{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b5270b",
   "metadata": {},
   "source": [
    "*Note: Place this notebook in STHN directory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96c211",
   "metadata": {},
   "source": [
    "# TGM (Satellite to Thermal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyYAML transformers timm h5py torch torchvision faiss-cpu einops opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed123af",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff5c63",
   "metadata": {},
   "source": [
    "[my_eval_pix2pix.py](./global_pipeline/my_eval_pix2pix.py)\n",
    "- [Input Image](./global_pipeline/my_eval_pix2pix.py:69)\n",
    "- [Output Image](./global_pipeline/my_eval_pix2pix.py:94)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e83058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py ./global_pipeline/my_eval_pix2pix.py --resume=\"./maps/models/TGM_nocontrast/best_model.pth\" --dataset_name=none --datasets_folder ./maps --G_net unet --GAN_upsample bilinear --GAN_resize 1024 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730a305",
   "metadata": {},
   "source": [
    "## Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f25a8",
   "metadata": {},
   "source": [
    "[my_TGM_folder2folder.py](./global_pipeline/my_TGM_folder2folder.py)\n",
    "- [Input Folder](./global_pipeline/my_TGM_folder2folder.py:296)\n",
    "- [Output Folder](./global_pipeline/my_TGM_folder2folder.py:297)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ecc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py ./global_pipeline/my_TGM_folder2folder.py --resume=\"./maps/models/TGM_nocontrast/best_model.pth\" --dataset_name=none --datasets_folder ./maps --G_net unet --GAN_upsample bilinear --GAN_resize 1024 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab645e10",
   "metadata": {},
   "source": [
    "# SHN (Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kornia scikit-image wandb openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9a674",
   "metadata": {},
   "source": [
    "[my_myevaluate.py](./local_pipeline/my_myevaluate.py)\n",
    "- [Input Image](./local_pipeline/my_myevaluate.py:118)\n",
    "- [Output Image](./local_pipeline/my_myevaluate.py:119)\n",
    "- [Output Excel](./local_pipeline/my_myevaluate.py:168)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a0073",
   "metadata": {},
   "source": [
    "## One-Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(\"cuda mem allocated:\", torch.cuda.memory_allocated() // (1024**2), \"MB\")\n",
    "print(\"cuda mem reserved:\",   torch.cuda.memory_reserved()   // (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -3.13  ./local_pipeline/my_myevaluate.py --dataset_name none --eval_model js_models/1536_one_stage/STHN.pth --val_positive_dist_threshold 512 --lev0 --database_size 1536 --corr_level 4 --test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc4231",
   "metadata": {},
   "source": [
    "## Two-Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd9d653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-01-02 14:38:12.037717021 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card1/device/vendor\"\u001b[m\n",
      "/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/local_pipeline/model/network.py:174: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "/home/rpl/.local/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/local_pipeline/model/network.py:203: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "(775.4905395507812, 677.66357421875)\n",
      "tensor([[[796.6096, 641.8204],\n",
      "         [749.1852, 681.2076],\n",
      "         [753.2419, 716.3058],\n",
      "         [802.9255, 671.3203]]])\n",
      "‚úÖ Done for image 0\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(850.1214599609375, 739.107177734375)\n",
      "tensor([[[856.2343, 673.1232],\n",
      "         [832.9622, 756.6013],\n",
      "         [793.9753, 738.5270],\n",
      "         [917.3139, 788.1772]]])\n",
      "‚úÖ Done for image 1\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(907.1723022460938, 857.2495727539062)\n",
      "tensor([[[1284.6091, 1084.1862],\n",
      "         [ 563.3802, 1128.1177],\n",
      "         [1031.6909,  546.8536],\n",
      "         [ 749.0090,  669.8411]]])\n",
      "‚úÖ Done for image 2\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(817.2608642578125, 664.91162109375)\n",
      "tensor([[[835.6095, 634.8091],\n",
      "         [800.2321, 660.2390],\n",
      "         [787.1755, 698.3130],\n",
      "         [846.0264, 666.2855]]])\n",
      "‚úÖ Done for image 3\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(829.4256591796875, 678.4275512695312)\n",
      "tensor([[[846.5228, 653.7074],\n",
      "         [813.4127, 671.2050],\n",
      "         [798.4418, 709.1852],\n",
      "         [859.3251, 679.6126]]])\n",
      "‚úÖ Done for image 4\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(884.8971557617188, 973.2920532226562)\n",
      "tensor([[[1246.7330, 1233.2896],\n",
      "         [ 540.7291, 1255.9551],\n",
      "         [1080.9639,  644.5662],\n",
      "         [ 671.1627,  759.3574]]])\n",
      "‚úÖ Done for image 5\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(857.962890625, 1015.4114990234375)\n",
      "tensor([[[1188.9357, 1252.8057],\n",
      "         [ 503.5670, 1280.7963],\n",
      "         [ 971.2971,  735.7178],\n",
      "         [ 768.0518,  792.3261]]])\n",
      "‚úÖ Done for image 6\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(869.7704467773438, 846.5718994140625)\n",
      "tensor([[[1244.7328, 1126.2000],\n",
      "         [ 509.5269, 1084.1979],\n",
      "         [1042.3364,  539.6538],\n",
      "         [ 682.4856,  636.2357]]])\n",
      "‚úÖ Done for image 7\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(804.8853759765625, 979.9716796875)\n",
      "tensor([[[1103.3864, 1211.8866],\n",
      "         [ 483.7242, 1210.0845],\n",
      "         [ 903.3618,  734.8285],\n",
      "         [ 729.0690,  763.0869]]])\n",
      "‚úÖ Done for image 8\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(916.9887084960938, 799.792724609375)\n",
      "tensor([[[958.3556, 771.5637],\n",
      "         [871.7661, 848.0787],\n",
      "         [860.5427, 759.6932],\n",
      "         [977.2903, 819.8353]]])\n",
      "‚úÖ Done for image 9\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(926.89990234375, 786.9139404296875)\n",
      "tensor([[[ 961.5037,  742.5598],\n",
      "         [ 887.5569,  848.4537],\n",
      "         [ 852.0458,  726.9764],\n",
      "         [1006.4933,  829.6659]]])\n",
      "‚úÖ Done for image 10\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(910.3310546875, 744.6273803710938)\n",
      "tensor([[[1412.3171, 1109.3883],\n",
      "         [ 430.6913, 1129.5988],\n",
      "         [1217.6447,  320.2975],\n",
      "         [ 580.6713,  419.2248]]])\n",
      "‚úÖ Done for image 11\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(809.6744384765625, 839.5730590820312)\n",
      "tensor([[[1216.1364, 1111.2119],\n",
      "         [ 409.8702, 1117.3448],\n",
      "         [1004.0137,  543.4775],\n",
      "         [ 608.6776,  586.2581]]])\n",
      "‚úÖ Done for image 12\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(932.2158203125, 774.65087890625)\n",
      "tensor([[[ 944.4106,  711.6503],\n",
      "         [ 927.4125,  813.9161],\n",
      "         [ 838.5397,  752.7449],\n",
      "         [1018.5006,  820.2924]]])\n",
      "‚úÖ Done for image 13\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(944.098876953125, 786.3200073242188)\n",
      "tensor([[[1406.4431, 1134.5464],\n",
      "         [ 479.4785, 1140.6298],\n",
      "         [1229.5559,  388.2345],\n",
      "         [ 660.9180,  481.8693]]])\n",
      "‚úÖ Done for image 14\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(967.004150390625, 1055.6053466796875)\n",
      "tensor([[[1503.8201, 1493.7524],\n",
      "         [ 459.9682, 1535.3679],\n",
      "         [1311.1710,  523.0972],\n",
      "         [ 593.0573,  670.2037]]])\n",
      "‚úÖ Done for image 15\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(885.994384765625, 744.998291015625)\n",
      "tensor([[[1127.9902,  901.9618],\n",
      "         [ 640.7758,  879.7675],\n",
      "         [ 977.4500,  560.3276],\n",
      "         [ 797.7616,  637.9364]]])\n",
      "‚úÖ Done for image 16\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(878.4722900390625, 764.3805541992188)\n",
      "tensor([[[916.7199, 726.9879],\n",
      "         [838.9116, 798.8212],\n",
      "         [824.1758, 745.0692],\n",
      "         [934.0818, 786.6438]]])\n",
      "‚úÖ Done for image 17\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(733.107421875, 763.875)\n",
      "tensor([[[1004.4181,  942.6809],\n",
      "         [ 464.5900,  972.0844],\n",
      "         [ 896.5128,  565.3211],\n",
      "         [ 566.9087,  575.4135]]])\n",
      "‚úÖ Done for image 18\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(879.405517578125, 777.9989624023438)\n",
      "tensor([[[916.4972, 756.8942],\n",
      "         [835.3262, 795.0356],\n",
      "         [820.9464, 752.0686],\n",
      "         [944.8523, 807.9973]]])\n",
      "‚úÖ Done for image 19\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(1087.957763671875, 816.5668334960938)\n",
      "tensor([[[1648.7744, 1224.5654],\n",
      "         [ 590.6870, 1322.4185],\n",
      "         [1400.3179,  292.2023],\n",
      "         [ 712.0519,  427.0812]]])\n",
      "‚úÖ Done for image 20\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(877.1248779296875, 793.834716796875)\n",
      "tensor([[[1034.1812,  882.4733],\n",
      "         [ 720.0906,  891.9816],\n",
      "         [ 895.5104,  685.0234],\n",
      "         [ 858.7172,  715.8606]]])\n",
      "‚úÖ Done for image 21\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(852.0482177734375, 693.150390625)\n",
      "tensor([[[870.1857, 677.6448],\n",
      "         [834.8560, 695.4458],\n",
      "         [816.2558, 713.7394],\n",
      "         [886.8951, 685.7715]]])\n",
      "‚úÖ Done for image 22\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(929.0196533203125, 798.30419921875)\n",
      "tensor([[[987.2161, 766.2794],\n",
      "         [879.1020, 851.9474],\n",
      "         [854.3365, 762.4249],\n",
      "         [995.4238, 812.5649]]])\n",
      "‚úÖ Done for image 23\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(845.56103515625, 797.964599609375)\n",
      "tensor([[[992.7256, 866.3904],\n",
      "         [683.2458, 890.5181],\n",
      "         [866.6836, 699.9047],\n",
      "         [839.5891, 735.0452]]])\n",
      "‚úÖ Done for image 24\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(806.6543579101562, 688.8546142578125)\n",
      "tensor([[[851.5842, 686.9225],\n",
      "         [751.4305, 728.3617],\n",
      "         [815.8354, 676.9922],\n",
      "         [807.7674, 663.1420]]])\n",
      "‚úÖ Done for image 25\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(884.4956665039062, 825.8695068359375)\n",
      "tensor([[[1138.6094,  985.2106],\n",
      "         [ 649.3907,  987.1204],\n",
      "         [ 954.9469,  636.4835],\n",
      "         [ 795.0357,  694.6636]]])\n",
      "‚úÖ Done for image 26\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(816.2996826171875, 805.1890869140625)\n",
      "tensor([[[989.5959, 898.2623],\n",
      "         [632.2944, 947.3084],\n",
      "         [883.1854, 661.0868],\n",
      "         [760.1230, 714.0985]]])\n",
      "‚úÖ Done for image 27\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(839.4873046875, 789.52197265625)\n",
      "tensor([[[1192.9844, 1023.4612],\n",
      "         [ 497.9037, 1071.0841],\n",
      "         [1031.8600,  512.0776],\n",
      "         [ 635.2013,  551.4648]]])\n",
      "‚úÖ Done for image 28\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(870.2002563476562, 725.439453125)\n",
      "tensor([[[856.0686, 687.5112],\n",
      "         [880.0884, 717.5518],\n",
      "         [803.8265, 743.0435],\n",
      "         [940.8177, 753.6514]]])\n",
      "‚úÖ Done for image 29\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(870.3236694335938, 825.4107666015625)\n",
      "tensor([[[1232.3147, 1076.4583],\n",
      "         [ 519.6213, 1082.4850],\n",
      "         [1039.0039,  547.2250],\n",
      "         [ 690.3547,  595.4747]]])\n",
      "‚úÖ Done for image 30\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(895.5321044921875, 733.041259765625)\n",
      "tensor([[[ 870.1062,  647.5105],\n",
      "         [ 918.0013,  701.8234],\n",
      "         [ 792.5762,  777.9240],\n",
      "         [1001.4447,  804.9071]]])\n",
      "‚úÖ Done for image 31\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(921.6939697265625, 730.8801879882812)\n",
      "tensor([[[ 932.5503,  685.8940],\n",
      "         [ 913.7529,  718.0366],\n",
      "         [ 836.7659,  743.7653],\n",
      "         [1003.7068,  775.8250]]])\n",
      "‚úÖ Done for image 32\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(921.134765625, 751.181884765625)\n",
      "tensor([[[1047.0032,  818.7455],\n",
      "         [ 811.0534,  823.1818],\n",
      "         [ 914.4431,  660.5817],\n",
      "         [ 912.0392,  702.2188]]])\n",
      "‚úÖ Done for image 33\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(848.34326171875, 701.5993041992188)\n",
      "tensor([[[876.2748, 696.3786],\n",
      "         [826.7651, 705.8510],\n",
      "         [819.6876, 717.2284],\n",
      "         [870.6454, 686.9392]]])\n",
      "‚úÖ Done for image 34\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(902.0764770507812, 786.4388427734375)\n",
      "tensor([[[933.7511, 776.3116],\n",
      "         [863.6239, 795.0287],\n",
      "         [842.0484, 757.5789],\n",
      "         [968.8826, 816.8362]]])\n",
      "‚úÖ Done for image 35\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(724.8600463867188, 839.8324584960938)\n",
      "tensor([[[1239.8087, 1250.2629],\n",
      "         [ 212.4919, 1251.0322],\n",
      "         [1075.3280,  413.3095],\n",
      "         [ 371.8117,  444.7252]]])\n",
      "‚úÖ Done for image 36\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(884.3831176757812, 801.5748291015625)\n",
      "tensor([[[1026.3306,  860.2130],\n",
      "         [ 744.7637,  920.3450],\n",
      "         [ 905.1337,  698.5988],\n",
      "         [ 861.3045,  727.1426]]])\n",
      "‚úÖ Done for image 37\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(900.06884765625, 787.9093017578125)\n",
      "tensor([[[1057.0486,  868.2151],\n",
      "         [ 746.6655,  901.0063],\n",
      "         [ 910.1666,  654.3136],\n",
      "         [ 886.3947,  728.1019]]])\n",
      "‚úÖ Done for image 38\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(909.5999145507812, 794.62060546875)\n",
      "tensor([[[942.4102, 749.0286],\n",
      "         [875.2432, 843.3574],\n",
      "         [825.5109, 755.5166],\n",
      "         [995.2355, 830.5798]]])\n",
      "‚úÖ Done for image 39\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(910.7454223632812, 790.35205078125)\n",
      "tensor([[[1052.0703,  874.8923],\n",
      "         [ 768.4836,  876.8835],\n",
      "         [ 923.6208,  676.4427],\n",
      "         [ 898.8069,  733.1896]]])\n",
      "‚úÖ Done for image 40\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(1195.48046875, 800.5371704101562)\n",
      "tensor([[[1788.4417, 1237.3767],\n",
      "         [ 648.0538, 1293.4331],\n",
      "         [1543.5881,  257.3809],\n",
      "         [ 801.8379,  413.9581]]])\n",
      "‚úÖ Done for image 41\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(885.4229736328125, 972.8516235351562)\n",
      "tensor([[[1188.8164, 1149.5774],\n",
      "         [ 599.4034, 1247.1869],\n",
      "         [ 938.7720,  711.5554],\n",
      "         [ 814.7001,  783.0870]]])\n",
      "‚úÖ Done for image 42\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(565.774169921875, 789.3438720703125)\n",
      "tensor([[[1280.9587, 1322.5880],\n",
      "         [-126.6498, 1416.7499],\n",
      "         [1048.2307,  211.0797],\n",
      "         [  60.5571,  206.9580]]])\n",
      "‚úÖ Done for image 43\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(906.6495361328125, 749.2052001953125)\n",
      "tensor([[[1086.9822,  862.1681],\n",
      "         [ 741.7162,  865.9085],\n",
      "         [ 930.2371,  590.3093],\n",
      "         [ 867.6627,  678.4349]]])\n",
      "‚úÖ Done for image 44\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(806.6927490234375, 792.9525756835938)\n",
      "tensor([[[971.8991, 873.1904],\n",
      "         [622.3746, 900.6682],\n",
      "         [833.5927, 655.3401],\n",
      "         [798.9049, 742.6115]]])\n",
      "‚úÖ Done for image 45\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(900.7115478515625, 791.865478515625)\n",
      "tensor([[[928.4211, 739.2487],\n",
      "         [872.5725, 827.5741],\n",
      "         [817.1088, 770.2148],\n",
      "         [984.7438, 830.4243]]])\n",
      "‚úÖ Done for image 46\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(910.630126953125, 791.2105712890625)\n",
      "tensor([[[950.6489, 744.8468],\n",
      "         [885.9575, 849.0676],\n",
      "         [823.0080, 750.4931],\n",
      "         [982.9059, 820.4348]]])\n",
      "‚úÖ Done for image 47\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(849.9653930664062, 836.5308837890625)\n",
      "tensor([[[1124.6271, 1012.9069],\n",
      "         [ 578.2103, 1035.1208],\n",
      "         [ 939.4397,  600.6931],\n",
      "         [ 757.5845,  697.4027]]])\n",
      "‚úÖ Done for image 48\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "(876.0580444335938, 807.7813720703125)\n",
      "tensor([[[1011.9340,  852.7694],\n",
      "         [ 751.2307,  881.6914],\n",
      "         [ 837.1794,  729.2950],\n",
      "         [ 903.8882,  767.3696]]])\n",
      "‚úÖ Done for image 49\n",
      "\n",
      "üìä Average processing time per image: 0.3994 sec\n",
      "üìÅ Saved all corner points to four_point_1_mul6.xlsx\n"
     ]
    }
   ],
   "source": [
    "!python ./local_pipeline/my_myevaluate.py --dataset_name none --eval_model js_models/1536_two_stages/STHN.pth --val_positive_dist_threshold 512 --lev0 --database_size 1536 --corr_level 4 --test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250e9d5",
   "metadata": {},
   "source": [
    "# TensorRT Conversion (STHN .pth ‚Üí ONNX ‚Üí TensorRT)\n",
    "- No `pycuda` is used (engine build via `trtexec`).\n",
    "- TensorRT `.engine` files are GPU/driver-specific: build them on the target machine (e.g., Jetson) with the same TensorRT version.\n",
    "- One-stage exports 1 ONNX/engine (coarse). Two-stage exports 2 ONNX/engines (coarse + fine). The crop/combine logic between stages remains in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd23a8f",
   "metadata": {},
   "source": [
    "## Export ONNX\n",
    "These commands export the weights from your `.pth` into ONNX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tools.export_sthn_onnx --pth \"js_models\\1536_one_stage\\STHN.pth\" --out_dir \"trt\\one_stage\" --stage coarse --resize_width 256 --corr_level 4 --iters 6 --database_size 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -3.13 -m tools.export_sthn_onnx --pth \"js_models\\1536_two_stages\\STHN.pth\" --out_dir \"trt\\two_stages\" --stage both --resize_width 256 --corr_level 4 --iters 6  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e0c0e",
   "metadata": {},
   "source": [
    "## Build TensorRT engines (requires TensorRT + `trtexec`)\n",
    "- Run these on the target GPU machine (e.g., Jetson).\n",
    "- If `trtexec` is not on your PATH, pass `--trtexec /full/path/to/trtexec`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f302449",
   "metadata": {},
   "source": [
    "### One-stage (coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ceecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tools.build_tensorrt_engine --onnx \"trt/one_stage/sthn_coarse.onnx\" --engine \"trt/one_stage/sthn_coarse_fp16.engine\" --fp16 --trtexec /usr/src/tensorrt/bin/trtexec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847e7e6",
   "metadata": {},
   "source": [
    "### Two-stage (coarse + fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adbc3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tools.build_tensorrt_engine --onnx \"trt/two_stages/sthn_coarse.onnx\" --engine \"trt/two_stages/sthn_coarse_fp16.engine\" --fp16 --trtexec /usr/src/tensorrt/bin/trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tools.build_tensorrt_engine --onnx \"trt/two_stages/sthn_fine.onnx\" --engine \"trt/two_stages/sthn_fine_fp16.engine\" --fp16 --trtexec /usr/src/tensorrt/bin/trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-stage (coarse)\n",
    "# !py -3.13 tools/build_tensorrt_engine.py --onnx trt/one_stage/sthn_coarse.onnx --engine trt/one_stage/sthn_coarse_fp16.engine --fp16 --shapes \"min=image1:1x3x256x256,image2:1x3x256x256;opt=image1:1x3x256x256,image2:1x3x256x256;max=image1:1x3x256x256,image2:1x3x256x256\"\n",
    "\n",
    "# # Two-stage (coarse + fine)\n",
    "# !py -3.13 tools/build_tensorrt_engine.py --onnx trt/two_stages/sthn_coarse.onnx --engine trt/two_stages/sthn_coarse_fp16.engine --fp16 --shapes \"min=image1:1x3x256x256,image2:1x3x256x256;opt=image1:1x3x256x256,image2:1x3x256x256;max=image1:1x3x256x256,image2:1x3x256x256\"\n",
    "# !py -3.13 tools/build_tensorrt_engine.py --onnx trt/two_stages/sthn_fine.onnx --engine trt/two_stages/sthn_fine_fp16.engine --fp16 --shapes \"min=image1_crop:1x3x256x256,image2:1x3x256x256;opt=image1_crop:1x3x256x256,image2:1x3x256x256;max=image1_crop:1x3x256x256,image2:1x3x256x256\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43be5c4",
   "metadata": {},
   "source": [
    "# TensorRT Inference (.engine)\n",
    "These use TensorRT engines directly (no `pycuda`).\n",
    "- One-stage: pass the coarse engine in `--eval_model`\n",
    "- Two-stage: pass both engines in `--eval_model` and `--eval_model_fine` and add `--two_stages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb31f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-01-02 11:53:46.974136665 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card1/device/vendor\"\u001b[m\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "[01/02/2026-11:53:47] [TRT] [E] [defaultAllocator.cpp::allocate::31] Error Code 1: Cuda Runtime (out of memory)\n",
      "[01/02/2026-11:53:47] [TRT] [E] [executionContext.cpp::ExecutionContext::565] Error Code 2: OutOfMemory (Requested size was 71320576 bytes.)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/./local_pipeline/my_myevaluate_trt.py\", line 107, in <module>\n",
      "    test(args)\n",
      "  File \"/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/./local_pipeline/my_myevaluate_trt.py\", line 42, in test\n",
      "    model = STHNTRT(\n",
      "  File \"/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/local_pipeline/model/network_trt.py\", line 86, in __init__\n",
      "    self.netG = IHNTRT(engine_coarse, image1_tensor_name=\"image1\")\n",
      "  File \"/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/local_pipeline/model/network_trt.py\", line 34, in __init__\n",
      "    self.engine = TrtEngine(engine_path)\n",
      "  File \"/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/local_pipeline/model/trt_engine.py\", line 49, in __init__\n",
      "    raise RuntimeError(f\"Failed to create TensorRT execution context: {engine_path}\")\n",
      "RuntimeError: Failed to create TensorRT execution context: trt/one_stage/sthn_coarse_fp16.engine\n"
     ]
    }
   ],
   "source": [
    "# One-stage TensorRT inference (coarse engine only)\n",
    "!python3 ./local_pipeline/my_myevaluate_trt.py --dataset_name none --eval_model trt/one_stage/sthn_coarse.engine --val_positive_dist_threshold 512 --lev0 --database_size 1536 --corr_level 4 --test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3065c15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(648.3709716796875, 732.7875366210938)\n",
      "tensor([[[439.5803, 512.3243],\n",
      "         [847.2935, 519.3723],\n",
      "         [453.0194, 950.8202],\n",
      "         [853.5908, 948.6334]]])\n",
      "‚úÖ Done for image 0\n",
      "(834.8838500976562, 753.2387084960938)\n",
      "tensor([[[ 489.9189,  414.2565],\n",
      "         [1162.2380,  405.4817],\n",
      "         [ 521.0660, 1089.0088],\n",
      "         [1166.3123, 1104.2079]]])\n",
      "‚úÖ Done for image 1\n",
      "(875.2380981445312, 712.0355224609375)\n",
      "tensor([[[799.8723, 634.5186],\n",
      "         [948.0734, 637.6548],\n",
      "         [805.1346, 788.9434],\n",
      "         [947.8721, 787.0253]]])\n",
      "‚úÖ Done for image 2\n",
      "(721.9774169921875, 722.1555786132812)\n",
      "tensor([[[ 386.0450,  380.0518],\n",
      "         [1044.4659,  397.2370],\n",
      "         [ 412.5507, 1058.5854],\n",
      "         [1044.8479, 1052.7480]]])\n",
      "‚úÖ Done for image 3\n",
      "(825.1072998046875, 750.33447265625)\n",
      "tensor([[[ 633.2064,  554.5209],\n",
      "         [1009.7736,  559.8675],\n",
      "         [ 645.2020,  945.6371],\n",
      "         [1012.2469,  941.3124]]])\n",
      "‚úÖ Done for image 4\n",
      "\n",
      "üìä Average processing time per image: 0.3184 sec\n",
      "üìÅ Saved all corner points to js_excels/predicted_trt.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Two-stage TensorRT inference (coarse + fine engines)\n",
    "!python3 ./local_pipeline/my_myevaluate_trt.py --dataset_name none --two_stages --eval_model trt/two_stages/sthn_coarse.engine --eval_model_fine trt/two_stages/sthn_fine.engine --val_positive_dist_threshold 512 --lev0 --database_size 1536 --corr_level 4 --test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
