{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b5270b",
   "metadata": {},
   "source": [
    "*Note: Place this notebook in STHN directory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96c211",
   "metadata": {},
   "source": [
    "# TGM (Satellite to Thermal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyYAML transformers timm h5py torch torchvision faiss-cpu einops opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed123af",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff5c63",
   "metadata": {},
   "source": [
    "[my_eval_pix2pix.py](./global_pipeline/my_eval_pix2pix.py)\n",
    "- [Input Image](./global_pipeline/my_eval_pix2pix.py:69)\n",
    "- [Output Image](./global_pipeline/my_eval_pix2pix.py:94)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e83058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py ./global_pipeline/my_eval_pix2pix.py --resume=\"./maps/models/TGM_nocontrast/best_model.pth\" --dataset_name=none --datasets_folder ./maps --G_net unet --GAN_upsample bilinear --GAN_resize 1024 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730a305",
   "metadata": {},
   "source": [
    "## Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f25a8",
   "metadata": {},
   "source": [
    "[my_TGM_folder2folder.py](./global_pipeline/my_TGM_folder2folder.py)\n",
    "- [Input Folder](./global_pipeline/my_TGM_folder2folder.py:296)\n",
    "- [Output Folder](./global_pipeline/my_TGM_folder2folder.py:297)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ecc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py ./global_pipeline/my_TGM_folder2folder.py --resume=\"./maps/models/TGM_nocontrast/best_model.pth\" --dataset_name=none --datasets_folder ./maps --G_net unet --GAN_upsample bilinear --GAN_resize 1024 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab645e10",
   "metadata": {},
   "source": [
    "# SHN (Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kornia scikit-image wandb openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9a674",
   "metadata": {},
   "source": [
    "[my_myevaluate.py](./local_pipeline/my_myevaluate.py)\n",
    "- [Input Image](./local_pipeline/my_myevaluate.py:118)\n",
    "- [Output Image](./local_pipeline/my_myevaluate.py:119)\n",
    "- [Output Excel](./local_pipeline/my_myevaluate.py:168)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a0073",
   "metadata": {},
   "source": [
    "## One-Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(\"cuda mem allocated:\", torch.cuda.memory_allocated() // (1024**2), \"MB\")\n",
    "print(\"cuda mem reserved:\",   torch.cuda.memory_reserved()   // (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -3.13  ./local_pipeline/my_myevaluate.py --dataset_name none --eval_model js_models/1536_one_stage/STHN.pth --val_positive_dist_threshold 512 --lev0 --database_size 1536 --corr_level 4 --test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc4231",
   "metadata": {},
   "source": [
    "## Two-Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -3.13 ./local_pipeline/my_myevaluate.py --dataset_name none --eval_model js_models/1536_two_stages/STHN.pth --val_positive_dist_threshold 512 --lev0 --database_size 1536 --corr_level 4 --test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250e9d5",
   "metadata": {},
   "source": [
    "# TensorRT Conversion (STHN .pth → ONNX → TensorRT)\n",
    "- No `pycuda` is used (engine build via `trtexec`).\n",
    "- TensorRT `.engine` files are GPU/driver-specific: build them on the target machine (e.g., Jetson) with the same TensorRT version.\n",
    "- One-stage exports 1 ONNX/engine (coarse). Two-stage exports 2 ONNX/engines (coarse + fine). The crop/combine logic between stages remains in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd23a8f",
   "metadata": {},
   "source": [
    "## Export ONNX\n",
    "These commands export the weights from your `.pth` into ONNX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387e607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `STHNCoarseONNX([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `STHNCoarseONNX([...]` with `torch.export.export(..., strict=False)`... ❌\n",
      "[torch.onnx] Obtain model graph for `STHNCoarseONNX([...]` with `torch.export.export(..., strict=True)`...\n",
      "[torch.onnx] Obtain model graph for `STHNCoarseONNX([...]` with `torch.export.export(..., strict=True)`... ❌\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0102 15:50:35.911000 16284 site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4319.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "W0102 15:50:38.158000 16284 site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:7942] Unable to find user code corresponding to {u0}\n",
      "W0102 15:50:38.304000 16284 site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:7942] Unable to find user code corresponding to {u1}\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1: \"f32[64, 3, 7, 7]\", arg1_1: \"f32[64]\", arg2_1: \"f32[64, 64, 3, 3]\", arg3_1: \"f32[64]\", arg4_1: \"f32[64, 64, 3, 3]\", arg5_1: \"f32[64]\", arg6_1: \"f32[64, 64, 1, 1]\", arg7_1: \"f32[64]\", arg8_1: \"f32[64, 64, 3, 3]\", arg9_1: \"f32[64]\", arg10_1: \"f32[64, 64, 3, 3]\", arg11_1: \"f32[64]\", arg12_1: \"f32[64, 64, 1, 1]\", arg13_1: \"f32[64]\", arg14_1: \"f32[96, 64, 3, 3]\", arg15_1: \"f32[96]\", arg16_1: \"f32[96, 96, 3, 3]\", arg17_1: \"f32[96]\", arg18_1: \"f32[96, 64, 1, 1]\", arg19_1: \"f32[96]\", arg20_1: \"f32[96, 96, 3, 3]\", arg21_1: \"f32[96]\", arg22_1: \"f32[96, 96, 3, 3]\", arg23_1: \"f32[96]\", arg24_1: \"f32[96, 96, 1, 1]\", arg25_1: \"f32[96]\", arg26_1: \"f32[256, 96, 1, 1]\", arg27_1: \"f32[256]\", arg28_1: \"f32[128, 326, 3, 3]\", arg29_1: \"f32[128]\", arg30_1: \"f32[128]\", arg31_1: \"f32[128]\", arg32_1: \"f32[128, 128, 3, 3]\", arg33_1: \"f32[128]\", arg34_1: \"f32[128]\", arg35_1: \"f32[128]\", arg36_1: \"f32[128, 128, 3, 3]\", arg37_1: \"f32[128]\", arg38_1: \"f32[128]\", arg39_1: \"f32[128]\", arg40_1: \"f32[128, 128, 3, 3]\", arg41_1: \"f32[128]\", arg42_1: \"f32[128]\", arg43_1: \"f32[128]\", arg44_1: \"f32[128, 128, 3, 3]\", arg45_1: \"f32[128]\", arg46_1: \"f32[128]\", arg47_1: \"f32[128]\", arg48_1: \"f32[128, 128, 3, 3]\", arg49_1: \"f32[128]\", arg50_1: \"f32[128]\", arg51_1: \"f32[128]\", arg52_1: \"f32[2, 128, 1, 1]\", arg53_1: \"f32[2]\", arg54_1: \"f32[1, 3, 256, 256]\", arg55_1: \"f32[1, 3, 256, 256]\"):\n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:108 in forward, code: self.imagenet_mean = torch.Tensor([0.485, 0.456, 0.406]).unsqueeze(0).unsqueeze(2).unsqueeze(3).to(image1.device)\n",
      "    _tensor_constant0: \"f32[3]\" = self._tensor_constant0\n",
      "    lift_fresh_copy: \"f32[3]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
      "    unsqueeze: \"f32[1, 3]\" = torch.ops.aten.unsqueeze.default(lift_fresh_copy, 0);  lift_fresh_copy = None\n",
      "    unsqueeze_1: \"f32[1, 3, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None\n",
      "    unsqueeze_2: \"f32[1, 3, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_1, 3);  unsqueeze_1 = None\n",
      "    to: \"f32[1, 3, 1, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_2, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  unsqueeze_2 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:109 in forward, code: self.imagenet_std = torch.Tensor([0.229, 0.224, 0.225]).unsqueeze(0).unsqueeze(2).unsqueeze(3).to(image1.device)\n",
      "    _tensor_constant1: \"f32[3]\" = self._tensor_constant1\n",
      "    lift_fresh_copy_1: \"f32[3]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None\n",
      "    unsqueeze_3: \"f32[1, 3]\" = torch.ops.aten.unsqueeze.default(lift_fresh_copy_1, 0);  lift_fresh_copy_1 = None\n",
      "    unsqueeze_4: \"f32[1, 3, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None\n",
      "    unsqueeze_5: \"f32[1, 3, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = None\n",
      "    to_1: \"f32[1, 3, 1, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_5, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  unsqueeze_5 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:110 in forward, code: image1 = (image1.contiguous() - self.imagenet_mean) / self.imagenet_std\n",
      "    sub: \"f32[1, 3, 256, 256]\" = torch.ops.aten.sub.Tensor(arg54_1, to);  arg54_1 = None\n",
      "    div: \"f32[1, 3, 256, 256]\" = torch.ops.aten.div.Tensor(sub, to_1);  sub = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:111 in forward, code: image2 = (image2.contiguous() - self.imagenet_mean) / self.imagenet_std\n",
      "    sub_1: \"f32[1, 3, 256, 256]\" = torch.ops.aten.sub.Tensor(arg55_1, to);  arg55_1 = to = None\n",
      "    div_1: \"f32[1, 3, 256, 256]\" = torch.ops.aten.div.Tensor(sub_1, to_1);  sub_1 = to_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:113 in forward, code: with _autocast(self.device.type, enabled=self.args.mixed_precision and self.device.type != \"cpu\"):\n",
      "    _enter_autocast = torch.amp.autocast_mode._enter_autocast('cpu', torch.bfloat16, False, False)\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d: \"f32[1, 64, 256, 256]\" = torch.ops.aten.conv2d.default(div, arg0_1, arg1_1, [1, 1], [3, 3]);  div = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm: \"f32[1, 64, 256, 256]\" = torch.ops.aten.instance_norm.default(conv2d, None, None, None, None, True, 0.1, 1e-05, True);  conv2d = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_: \"f32[1, 64, 256, 256]\" = torch.ops.aten.relu_.default(instance_norm);  instance_norm = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:255 in forward, code: x = F.max_pool2d(x, 2, stride=2)\n",
      "    max_pool2d: \"f32[1, 64, 128, 128]\" = torch.ops.aten.max_pool2d.default(relu_, [2, 2], [2, 2]);  relu_ = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_1: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(max_pool2d, arg2_1, arg3_1, [1, 1], [1, 1])\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_1: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_1, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__1: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_1);  instance_norm_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_2: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__1, arg4_1, arg5_1, [1, 1], [1, 1]);  relu__1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_2: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_2, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__2: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_2);  instance_norm_2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_3: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(max_pool2d, arg6_1, arg7_1);  max_pool2d = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_3: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_3, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_3 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add: \"f32[1, 64, 128, 128]\" = torch.ops.aten.add.Tensor(instance_norm_3, relu__2);  instance_norm_3 = relu__2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__3: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(add);  add = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_4: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__3, arg8_1, arg9_1, [1, 1], [1, 1])\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_4: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_4, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__4: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_4);  instance_norm_4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_5: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__4, arg10_1, arg11_1, [1, 1], [1, 1]);  relu__4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_5: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_5, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_5 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__5: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_5);  instance_norm_5 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_6: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__3, arg12_1, arg13_1);  relu__3 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_6: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_6, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_6 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_1: \"f32[1, 64, 128, 128]\" = torch.ops.aten.add.Tensor(instance_norm_6, relu__5);  instance_norm_6 = relu__5 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__6: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(add_1);  add_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:257 in forward, code: x = F.max_pool2d(x, 2, stride=2)\n",
      "    max_pool2d_1: \"f32[1, 64, 64, 64]\" = torch.ops.aten.max_pool2d.default(relu__6, [2, 2], [2, 2]);  relu__6 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_7: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(max_pool2d_1, arg14_1, arg15_1, [1, 1], [1, 1])\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_7: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_7, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_7 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__7: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_7);  instance_norm_7 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_8: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__7, arg16_1, arg17_1, [1, 1], [1, 1]);  relu__7 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_8: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_8, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_8 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__8: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_8);  instance_norm_8 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_9: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(max_pool2d_1, arg18_1, arg19_1);  max_pool2d_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_9: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_9, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_9 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_2: \"f32[1, 96, 64, 64]\" = torch.ops.aten.add.Tensor(instance_norm_9, relu__8);  instance_norm_9 = relu__8 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__9: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(add_2);  add_2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_10: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__9, arg20_1, arg21_1, [1, 1], [1, 1])\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_10: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_10, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_10 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__10: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_10);  instance_norm_10 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_11: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__10, arg22_1, arg23_1, [1, 1], [1, 1]);  relu__10 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_11: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_11, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_11 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__11: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_11);  instance_norm_11 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_12: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__9, arg24_1, arg25_1);  relu__9 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_12: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_12, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_12 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_3: \"f32[1, 96, 64, 64]\" = torch.ops.aten.add.Tensor(instance_norm_12, relu__11);  instance_norm_12 = relu__11 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__12: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(add_3);  add_3 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_13: \"f32[1, 256, 64, 64]\" = torch.ops.aten.conv2d.default(relu__12, arg26_1, arg27_1);  relu__12 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_14: \"f32[1, 64, 256, 256]\" = torch.ops.aten.conv2d.default(div_1, arg0_1, arg1_1, [1, 1], [3, 3]);  div_1 = arg0_1 = arg1_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_13: \"f32[1, 64, 256, 256]\" = torch.ops.aten.instance_norm.default(conv2d_14, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_14 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__13: \"f32[1, 64, 256, 256]\" = torch.ops.aten.relu_.default(instance_norm_13);  instance_norm_13 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:255 in forward, code: x = F.max_pool2d(x, 2, stride=2)\n",
      "    max_pool2d_2: \"f32[1, 64, 128, 128]\" = torch.ops.aten.max_pool2d.default(relu__13, [2, 2], [2, 2]);  relu__13 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_15: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(max_pool2d_2, arg2_1, arg3_1, [1, 1], [1, 1]);  arg2_1 = arg3_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_14: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_15, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_15 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__14: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_14);  instance_norm_14 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_16: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__14, arg4_1, arg5_1, [1, 1], [1, 1]);  relu__14 = arg4_1 = arg5_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_15: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_16, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_16 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__15: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_15);  instance_norm_15 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_17: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(max_pool2d_2, arg6_1, arg7_1);  max_pool2d_2 = arg6_1 = arg7_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_16: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_17, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_17 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_4: \"f32[1, 64, 128, 128]\" = torch.ops.aten.add.Tensor(instance_norm_16, relu__15);  instance_norm_16 = relu__15 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__16: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(add_4);  add_4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_18: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__16, arg8_1, arg9_1, [1, 1], [1, 1]);  arg8_1 = arg9_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_17: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_18, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_18 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__17: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_17);  instance_norm_17 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_19: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__17, arg10_1, arg11_1, [1, 1], [1, 1]);  relu__17 = arg10_1 = arg11_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_18: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_19, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_19 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__18: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_18);  instance_norm_18 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_20: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__16, arg12_1, arg13_1);  relu__16 = arg12_1 = arg13_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_19: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_20, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_20 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_5: \"f32[1, 64, 128, 128]\" = torch.ops.aten.add.Tensor(instance_norm_19, relu__18);  instance_norm_19 = relu__18 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__19: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(add_5);  add_5 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:257 in forward, code: x = F.max_pool2d(x, 2, stride=2)\n",
      "    max_pool2d_3: \"f32[1, 64, 64, 64]\" = torch.ops.aten.max_pool2d.default(relu__19, [2, 2], [2, 2]);  relu__19 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_21: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(max_pool2d_3, arg14_1, arg15_1, [1, 1], [1, 1]);  arg14_1 = arg15_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_20: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_21, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_21 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__20: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_20);  instance_norm_20 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_22: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__20, arg16_1, arg17_1, [1, 1], [1, 1]);  relu__20 = arg16_1 = arg17_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_21: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_22, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_22 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__21: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_21);  instance_norm_21 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_23: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(max_pool2d_3, arg18_1, arg19_1);  max_pool2d_3 = arg18_1 = arg19_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_22: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_23, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_23 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_6: \"f32[1, 96, 64, 64]\" = torch.ops.aten.add.Tensor(instance_norm_22, relu__21);  instance_norm_22 = relu__21 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__22: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(add_6);  add_6 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_24: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__22, arg20_1, arg21_1, [1, 1], [1, 1]);  arg20_1 = arg21_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_23: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_24, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_24 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__23: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_23);  instance_norm_23 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_25: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__23, arg22_1, arg23_1, [1, 1], [1, 1]);  relu__23 = arg22_1 = arg23_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_24: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_25, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_25 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__24: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_24);  instance_norm_24 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_26: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__22, arg24_1, arg25_1);  relu__22 = arg24_1 = arg25_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_25: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_26, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_26 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_7: \"f32[1, 96, 64, 64]\" = torch.ops.aten.add.Tensor(instance_norm_25, relu__24);  instance_norm_25 = relu__24 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__25: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(add_7);  add_7 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_27: \"f32[1, 256, 64, 64]\" = torch.ops.aten.conv2d.default(relu__25, arg26_1, arg27_1);  relu__25 = arg26_1 = arg27_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:113 in forward, code: with _autocast(self.device.type, enabled=self.args.mixed_precision and self.device.type != \"cpu\"):\n",
      "    _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:122 in forward, code: fmap1 = fmap1_64.float()\n",
      "    to_2: \"f32[1, 256, 64, 64]\" = torch.ops.aten.to.dtype(conv2d_13, torch.float32);  conv2d_13 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:123 in forward, code: fmap2 = fmap2_64.float()\n",
      "    to_3: \"f32[1, 256, 64, 64]\" = torch.ops.aten.to.dtype(conv2d_27, torch.float32);  conv2d_27 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:125 in forward, code: corr_fn = CorrBlock(fmap1, fmap2, num_levels=corr_level, radius=corr_radius)\n",
      "    view: \"f32[1, 256, 4096]\" = torch.ops.aten.view.default(to_2, [1, 256, 4096]);  to_2 = None\n",
      "    view_1: \"f32[1, 256, 4096]\" = torch.ops.aten.view.default(to_3, [1, 256, 4096]);  to_3 = None\n",
      "    transpose: \"f32[1, 4096, 256]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
      "    matmul: \"f32[1, 4096, 4096]\" = torch.ops.aten.matmul.default(transpose, view_1);  transpose = view_1 = None\n",
      "    relu: \"f32[1, 4096, 4096]\" = torch.ops.aten.relu.default(matmul);  matmul = None\n",
      "    view_2: \"f32[1, 64, 64, 1, 64, 64]\" = torch.ops.aten.view.default(relu, [1, 64, 64, 1, 64, 64]);  relu = None\n",
      "    reshape: \"f32[4096, 1, 64, 64]\" = torch.ops.aten.reshape.default(view_2, [4096, 1, 64, 64]);  view_2 = None\n",
      "    avg_pool2d: \"f32[4096, 1, 32, 32]\" = torch.ops.aten.avg_pool2d.default(reshape, [2, 2], [2, 2])\n",
      "    avg_pool2d_1: \"f32[4096, 1, 16, 16]\" = torch.ops.aten.avg_pool2d.default(avg_pool2d, [2, 2], [2, 2])\n",
      "    avg_pool2d_2: \"f32[4096, 1, 8, 8]\" = torch.ops.aten.avg_pool2d.default(avg_pool2d_1, [2, 2], [2, 2])\n",
      "    linspace: \"f32[9]\" = torch.ops.aten.linspace.default(-4, 4, 9, device = device(type='cpu'), pin_memory = False)\n",
      "    linspace_1: \"f32[9]\" = torch.ops.aten.linspace.default(-4, 4, 9, device = device(type='cpu'), pin_memory = False)\n",
      "    meshgrid = torch.ops.aten.meshgrid.default([linspace_1, linspace]);  linspace_1 = linspace = None\n",
      "    getitem: \"f32[9, 9]\" = meshgrid[0]\n",
      "    getitem_1: \"f32[9, 9]\" = meshgrid[1];  meshgrid = None\n",
      "    stack: \"f32[9, 9, 2]\" = torch.ops.aten.stack.default([getitem, getitem_1], -1);  getitem = getitem_1 = None\n",
      "    to_4: \"f32[9, 9, 2]\" = torch.ops.aten.to.dtype_layout(stack, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  stack = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:126 in forward, code: coords0, coords1 = self.initialize_flow_4(image1)\n",
      "    arange: \"i64[64]\" = torch.ops.aten.arange.default(64, device = device(type='cpu'), pin_memory = False)\n",
      "    arange_1: \"i64[64]\" = torch.ops.aten.arange.default(64, device = device(type='cpu'), pin_memory = False)\n",
      "    meshgrid_1 = torch.ops.aten.meshgrid.default([arange, arange_1]);  arange = arange_1 = None\n",
      "    getitem_2: \"i64[64, 64]\" = meshgrid_1[0]\n",
      "    getitem_3: \"i64[64, 64]\" = meshgrid_1[1];  meshgrid_1 = None\n",
      "    stack_1: \"i64[2, 64, 64]\" = torch.ops.aten.stack.default([getitem_3, getitem_2]);  getitem_3 = getitem_2 = None\n",
      "    to_5: \"f32[2, 64, 64]\" = torch.ops.aten.to.dtype(stack_1, torch.float32);  stack_1 = None\n",
      "    unsqueeze_6: \"f32[1, 2, 64, 64]\" = torch.ops.aten.unsqueeze.default(to_5, 0);  to_5 = None\n",
      "    expand: \"f32[1, 2, 64, 64]\" = torch.ops.aten.expand.default(unsqueeze_6, [1, -1, -1, -1]);  unsqueeze_6 = None\n",
      "    to_6: \"f32[1, 2, 64, 64]\" = torch.ops.aten.to.dtype_layout(expand, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  expand = None\n",
      "    arange_2: \"i64[64]\" = torch.ops.aten.arange.default(64, device = device(type='cpu'), pin_memory = False)\n",
      "    arange_3: \"i64[64]\" = torch.ops.aten.arange.default(64, device = device(type='cpu'), pin_memory = False)\n",
      "    meshgrid_2 = torch.ops.aten.meshgrid.default([arange_2, arange_3]);  arange_2 = arange_3 = None\n",
      "    getitem_4: \"i64[64, 64]\" = meshgrid_2[0]\n",
      "    getitem_5: \"i64[64, 64]\" = meshgrid_2[1];  meshgrid_2 = None\n",
      "    stack_2: \"i64[2, 64, 64]\" = torch.ops.aten.stack.default([getitem_5, getitem_4]);  getitem_5 = getitem_4 = None\n",
      "    to_7: \"f32[2, 64, 64]\" = torch.ops.aten.to.dtype(stack_2, torch.float32);  stack_2 = None\n",
      "    unsqueeze_7: \"f32[1, 2, 64, 64]\" = torch.ops.aten.unsqueeze.default(to_7, 0);  to_7 = None\n",
      "    expand_1: \"f32[1, 2, 64, 64]\" = torch.ops.aten.expand.default(unsqueeze_7, [1, -1, -1, -1]);  unsqueeze_7 = None\n",
      "    to_8: \"f32[1, 2, 64, 64]\" = torch.ops.aten.to.dtype_layout(expand_1, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  expand_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:129 in forward, code: four_point_disp = torch.zeros((sz[0], 2, 2, 2)).to(fmap1.device)\n",
      "    zeros: \"f32[1, 2, 2, 2]\" = torch.ops.aten.zeros.default([1, 2, 2, 2], device = device(type='cpu'), pin_memory = False)\n",
      "    to_9: \"f32[1, 2, 2, 2]\" = torch.ops.aten.to.dtype_layout(zeros, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  zeros = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:132 in forward, code: corr = corr_fn(coords1)\n",
      "    permute: \"f32[1, 64, 64, 2]\" = torch.ops.aten.permute.default(to_8, [0, 2, 3, 1])\n",
      "    reshape_1: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.reshape.default(permute, [4096, 1, 1, 2])\n",
      "    div_2: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.div.Tensor(reshape_1, 1);  reshape_1 = None\n",
      "    view_3: \"f32[1, 9, 9, 2]\" = torch.ops.aten.view.default(to_4, [1, 9, 9, 2])\n",
      "    add_8: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.add.Tensor(div_2, view_3);  div_2 = view_3 = None\n",
      "    split_with_sizes = torch.ops.aten.split_with_sizes.default(add_8, [1, 1], -1);  add_8 = None\n",
      "    getitem_6: \"f32[4096, 9, 9, 1]\" = split_with_sizes[0]\n",
      "    getitem_7: \"f32[4096, 9, 9, 1]\" = split_with_sizes[1];  split_with_sizes = None\n",
      "    mul: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_6, 2);  getitem_6 = None\n",
      "    div_3: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul, 63);  mul = None\n",
      "    sub_2: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_3, 1);  div_3 = None\n",
      "    mul_1: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_7, 2);  getitem_7 = None\n",
      "    div_4: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_1, 63);  mul_1 = None\n",
      "    sub_3: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_4, 1);  div_4 = None\n",
      "    cat: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.cat.default([sub_2, sub_3], -1);  sub_2 = sub_3 = None\n",
      "    grid_sampler: \"f32[4096, 1, 9, 9]\" = torch.ops.aten.grid_sampler.default(reshape, cat, 0, 0, True);  reshape = cat = None\n",
      "    view_4: \"f32[1, 64, 64, 81]\" = torch.ops.aten.view.default(grid_sampler, [1, 64, 64, -1]);  grid_sampler = None\n",
      "    reshape_2: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.reshape.default(permute, [4096, 1, 1, 2])\n",
      "    div_5: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.div.Tensor(reshape_2, 2);  reshape_2 = None\n",
      "    view_5: \"f32[1, 9, 9, 2]\" = torch.ops.aten.view.default(to_4, [1, 9, 9, 2])\n",
      "    add_9: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.add.Tensor(div_5, view_5);  div_5 = view_5 = None\n",
      "    split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(add_9, [1, 1], -1);  add_9 = None\n",
      "    getitem_8: \"f32[4096, 9, 9, 1]\" = split_with_sizes_1[0]\n",
      "    getitem_9: \"f32[4096, 9, 9, 1]\" = split_with_sizes_1[1];  split_with_sizes_1 = None\n",
      "    mul_2: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_8, 2);  getitem_8 = None\n",
      "    div_6: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_2, 31);  mul_2 = None\n",
      "    sub_4: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_6, 1);  div_6 = None\n",
      "    mul_3: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_9, 2);  getitem_9 = None\n",
      "    div_7: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_3, 31);  mul_3 = None\n",
      "    sub_5: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_7, 1);  div_7 = None\n",
      "    cat_1: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.cat.default([sub_4, sub_5], -1);  sub_4 = sub_5 = None\n",
      "    grid_sampler_1: \"f32[4096, 1, 9, 9]\" = torch.ops.aten.grid_sampler.default(avg_pool2d, cat_1, 0, 0, True);  avg_pool2d = cat_1 = None\n",
      "    view_6: \"f32[1, 64, 64, 81]\" = torch.ops.aten.view.default(grid_sampler_1, [1, 64, 64, -1]);  grid_sampler_1 = None\n",
      "    reshape_3: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.reshape.default(permute, [4096, 1, 1, 2])\n",
      "    div_8: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.div.Tensor(reshape_3, 4);  reshape_3 = None\n",
      "    view_7: \"f32[1, 9, 9, 2]\" = torch.ops.aten.view.default(to_4, [1, 9, 9, 2])\n",
      "    add_10: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.add.Tensor(div_8, view_7);  div_8 = view_7 = None\n",
      "    split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(add_10, [1, 1], -1);  add_10 = None\n",
      "    getitem_10: \"f32[4096, 9, 9, 1]\" = split_with_sizes_2[0]\n",
      "    getitem_11: \"f32[4096, 9, 9, 1]\" = split_with_sizes_2[1];  split_with_sizes_2 = None\n",
      "    mul_4: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_10, 2);  getitem_10 = None\n",
      "    div_9: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_4, 15);  mul_4 = None\n",
      "    sub_6: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_9, 1);  div_9 = None\n",
      "    mul_5: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_11, 2);  getitem_11 = None\n",
      "    div_10: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_5, 15);  mul_5 = None\n",
      "    sub_7: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_10, 1);  div_10 = None\n",
      "    cat_2: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.cat.default([sub_6, sub_7], -1);  sub_6 = sub_7 = None\n",
      "    grid_sampler_2: \"f32[4096, 1, 9, 9]\" = torch.ops.aten.grid_sampler.default(avg_pool2d_1, cat_2, 0, 0, True);  avg_pool2d_1 = cat_2 = None\n",
      "    view_8: \"f32[1, 64, 64, 81]\" = torch.ops.aten.view.default(grid_sampler_2, [1, 64, 64, -1]);  grid_sampler_2 = None\n",
      "    reshape_4: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.reshape.default(permute, [4096, 1, 1, 2]);  permute = None\n",
      "    div_11: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.div.Tensor(reshape_4, 8);  reshape_4 = None\n",
      "    view_9: \"f32[1, 9, 9, 2]\" = torch.ops.aten.view.default(to_4, [1, 9, 9, 2]);  to_4 = None\n",
      "    add_11: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.add.Tensor(div_11, view_9);  div_11 = view_9 = None\n",
      "    split_with_sizes_3 = torch.ops.aten.split_with_sizes.default(add_11, [1, 1], -1);  add_11 = None\n",
      "    getitem_12: \"f32[4096, 9, 9, 1]\" = split_with_sizes_3[0]\n",
      "    getitem_13: \"f32[4096, 9, 9, 1]\" = split_with_sizes_3[1];  split_with_sizes_3 = None\n",
      "    mul_6: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_12, 2);  getitem_12 = None\n",
      "    div_12: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_6, 7);  mul_6 = None\n",
      "    sub_8: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_12, 1);  div_12 = None\n",
      "    mul_7: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_13, 2);  getitem_13 = None\n",
      "    div_13: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_7, 7);  mul_7 = None\n",
      "    sub_9: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_13, 1);  div_13 = None\n",
      "    cat_3: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.cat.default([sub_8, sub_9], -1);  sub_8 = sub_9 = None\n",
      "    grid_sampler_3: \"f32[4096, 1, 9, 9]\" = torch.ops.aten.grid_sampler.default(avg_pool2d_2, cat_3, 0, 0, True);  avg_pool2d_2 = cat_3 = None\n",
      "    view_10: \"f32[1, 64, 64, 81]\" = torch.ops.aten.view.default(grid_sampler_3, [1, 64, 64, -1]);  grid_sampler_3 = None\n",
      "    cat_4: \"f32[1, 64, 64, 324]\" = torch.ops.aten.cat.default([view_4, view_6, view_8, view_10], -1);  view_4 = view_6 = view_8 = view_10 = None\n",
      "    permute_1: \"f32[1, 324, 64, 64]\" = torch.ops.aten.permute.default(cat_4, [0, 3, 1, 2]);  cat_4 = None\n",
      "    contiguous: \"f32[1, 324, 64, 64]\" = torch.ops.aten.contiguous.default(permute_1);  permute_1 = None\n",
      "    to_10: \"f32[1, 324, 64, 64]\" = torch.ops.aten.to.dtype(contiguous, torch.float32);  contiguous = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:133 in forward, code: flow = coords1 - coords0\n",
      "    sub_10: \"f32[1, 2, 64, 64]\" = torch.ops.aten.sub.Tensor(to_8, to_6);  to_8 = to_6 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:134 in forward, code: with _autocast(self.device.type, enabled=self.args.mixed_precision and self.device.type != \"cpu\"):\n",
      "    _enter_autocast_1 = torch.amp.autocast_mode._enter_autocast('cpu', torch.bfloat16, False, False)\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\update.py:327 in forward, code: delta_flow = self.cnn(torch.cat((corr, flow), dim=1))\n",
      "    cat_5: \"f32[1, 326, 64, 64]\" = torch.ops.aten.cat.default([to_10, sub_10], 1);  to_10 = sub_10 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_28: \"f32[1, 128, 64, 64]\" = torch.ops.aten.conv2d.default(cat_5, arg28_1, arg29_1, [1, 1], [1, 1]);  cat_5 = arg28_1 = arg29_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm: \"f32[1, 128, 64, 64]\" = torch.ops.aten.group_norm.default(conv2d_28, 16, arg30_1, arg31_1);  conv2d_28 = arg30_1 = arg31_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_1: \"f32[1, 128, 64, 64]\" = torch.ops.aten.relu.default(group_norm);  group_norm = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
      "    max_pool2d_4: \"f32[1, 128, 32, 32]\" = torch.ops.aten.max_pool2d.default(relu_1, [2, 2], [2, 2]);  relu_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_29: \"f32[1, 128, 32, 32]\" = torch.ops.aten.conv2d.default(max_pool2d_4, arg32_1, arg33_1, [1, 1], [1, 1]);  max_pool2d_4 = arg32_1 = arg33_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm_1: \"f32[1, 128, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_29, 16, arg34_1, arg35_1);  conv2d_29 = arg34_1 = arg35_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_2: \"f32[1, 128, 32, 32]\" = torch.ops.aten.relu.default(group_norm_1);  group_norm_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
      "    max_pool2d_5: \"f32[1, 128, 16, 16]\" = torch.ops.aten.max_pool2d.default(relu_2, [2, 2], [2, 2]);  relu_2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_30: \"f32[1, 128, 16, 16]\" = torch.ops.aten.conv2d.default(max_pool2d_5, arg36_1, arg37_1, [1, 1], [1, 1]);  max_pool2d_5 = arg36_1 = arg37_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm_2: \"f32[1, 128, 16, 16]\" = torch.ops.aten.group_norm.default(conv2d_30, 16, arg38_1, arg39_1);  conv2d_30 = arg38_1 = arg39_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_3: \"f32[1, 128, 16, 16]\" = torch.ops.aten.relu.default(group_norm_2);  group_norm_2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
      "    max_pool2d_6: \"f32[1, 128, 8, 8]\" = torch.ops.aten.max_pool2d.default(relu_3, [2, 2], [2, 2]);  relu_3 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_31: \"f32[1, 128, 8, 8]\" = torch.ops.aten.conv2d.default(max_pool2d_6, arg40_1, arg41_1, [1, 1], [1, 1]);  max_pool2d_6 = arg40_1 = arg41_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm_3: \"f32[1, 128, 8, 8]\" = torch.ops.aten.group_norm.default(conv2d_31, 16, arg42_1, arg43_1);  conv2d_31 = arg42_1 = arg43_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_4: \"f32[1, 128, 8, 8]\" = torch.ops.aten.relu.default(group_norm_3);  group_norm_3 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
      "    max_pool2d_7: \"f32[1, 128, 4, 4]\" = torch.ops.aten.max_pool2d.default(relu_4, [2, 2], [2, 2]);  relu_4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_32: \"f32[1, 128, 4, 4]\" = torch.ops.aten.conv2d.default(max_pool2d_7, arg44_1, arg45_1, [1, 1], [1, 1]);  max_pool2d_7 = arg44_1 = arg45_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm_4: \"f32[1, 128, 4, 4]\" = torch.ops.aten.group_norm.default(conv2d_32, 16, arg46_1, arg47_1);  conv2d_32 = arg46_1 = arg47_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_5: \"f32[1, 128, 4, 4]\" = torch.ops.aten.relu.default(group_norm_4);  group_norm_4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
      "    max_pool2d_8: \"f32[1, 128, 2, 2]\" = torch.ops.aten.max_pool2d.default(relu_5, [2, 2], [2, 2]);  relu_5 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_33: \"f32[1, 128, 2, 2]\" = torch.ops.aten.conv2d.default(max_pool2d_8, arg48_1, arg49_1, [1, 1], [1, 1]);  max_pool2d_8 = arg48_1 = arg49_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm_5: \"f32[1, 128, 2, 2]\" = torch.ops.aten.group_norm.default(conv2d_33, 16, arg50_1, arg51_1);  conv2d_33 = arg50_1 = arg51_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_6: \"f32[1, 128, 2, 2]\" = torch.ops.aten.relu.default(group_norm_5);  group_norm_5 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_34: \"f32[1, 2, 2, 2]\" = torch.ops.aten.conv2d.default(relu_6, arg52_1, arg53_1);  relu_6 = arg52_1 = arg53_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:134 in forward, code: with _autocast(self.device.type, enabled=self.args.mixed_precision and self.device.type != \"cpu\"):\n",
      "    _exit_autocast_1 = torch.amp.autocast_mode._exit_autocast(_enter_autocast_1);  _enter_autocast_1 = _exit_autocast_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:142 in forward, code: four_point_disp = four_point_disp + delta_four_point\n",
      "    add_12: \"f32[1, 2, 2, 2]\" = torch.ops.aten.add.Tensor(to_9, conv2d_34);  conv2d_34 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:143 in forward, code: coords1 = self.get_flow_now_4(four_point_disp)\n",
      "    div_14: \"f32[1, 2, 2, 2]\" = torch.ops.aten.div.Tensor(add_12, 4);  add_12 = None\n",
      "    zeros_1: \"f32[2, 2, 2]\" = torch.ops.aten.zeros.default([2, 2, 2], device = device(type='cpu'), pin_memory = False)\n",
      "    to_11: \"f32[2, 2, 2]\" = torch.ops.aten.to.dtype_layout(zeros_1, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  zeros_1 = None\n",
      "    _tensor_constant2: \"f32[2]\" = self._tensor_constant2\n",
      "    lift_fresh_copy_2: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None\n",
      "    select: \"f32[2, 2]\" = torch.ops.aten.select.int(to_11, 1, 0)\n",
      "    select_1: \"f32[2]\" = torch.ops.aten.select.int(select, 1, 0);  select = None\n",
      "    copy_: \"f32[2]\" = torch.ops.aten.copy_.default(select_1, lift_fresh_copy_2);  select_1 = lift_fresh_copy_2 = copy_ = None\n",
      "    _tensor_constant3: \"f32[2]\" = self._tensor_constant3\n",
      "    lift_fresh_copy_3: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None\n",
      "    select_2: \"f32[2, 2]\" = torch.ops.aten.select.int(to_11, 1, 0)\n",
      "    select_3: \"f32[2]\" = torch.ops.aten.select.int(select_2, 1, 1);  select_2 = None\n",
      "    copy__1: \"f32[2]\" = torch.ops.aten.copy_.default(select_3, lift_fresh_copy_3);  select_3 = lift_fresh_copy_3 = copy__1 = None\n",
      "    _tensor_constant4: \"f32[2]\" = self._tensor_constant4\n",
      "    lift_fresh_copy_4: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None\n",
      "    select_4: \"f32[2, 2]\" = torch.ops.aten.select.int(to_11, 1, 1)\n",
      "    select_5: \"f32[2]\" = torch.ops.aten.select.int(select_4, 1, 0);  select_4 = None\n",
      "    copy__2: \"f32[2]\" = torch.ops.aten.copy_.default(select_5, lift_fresh_copy_4);  select_5 = lift_fresh_copy_4 = copy__2 = None\n",
      "    _tensor_constant5: \"f32[2]\" = self._tensor_constant5\n",
      "    lift_fresh_copy_5: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None\n",
      "    select_6: \"f32[2, 2]\" = torch.ops.aten.select.int(to_11, 1, 1)\n",
      "    select_7: \"f32[2]\" = torch.ops.aten.select.int(select_6, 1, 1);  select_6 = None\n",
      "    copy__3: \"f32[2]\" = torch.ops.aten.copy_.default(select_7, lift_fresh_copy_5);  select_7 = lift_fresh_copy_5 = copy__3 = None\n",
      "    unsqueeze_8: \"f32[1, 2, 2, 2]\" = torch.ops.aten.unsqueeze.default(to_11, 0);  to_11 = None\n",
      "    repeat: \"f32[1, 2, 2, 2]\" = torch.ops.aten.repeat.default(unsqueeze_8, [1, 1, 1, 1]);  unsqueeze_8 = None\n",
      "    add_13: \"f32[1, 2, 2, 2]\" = torch.ops.aten.add.Tensor(repeat, div_14);  div_14 = None\n",
      "    flatten: \"f32[1, 2, 4]\" = torch.ops.aten.flatten.using_ints(repeat, 2);  repeat = None\n",
      "    permute_2: \"f32[1, 4, 2]\" = torch.ops.aten.permute.default(flatten, [0, 2, 1]);  flatten = None\n",
      "    contiguous_1: \"f32[1, 4, 2]\" = torch.ops.aten.contiguous.default(permute_2);  permute_2 = None\n",
      "    flatten_1: \"f32[1, 2, 4]\" = torch.ops.aten.flatten.using_ints(add_13, 2);  add_13 = None\n",
      "    permute_3: \"f32[1, 4, 2]\" = torch.ops.aten.permute.default(flatten_1, [0, 2, 1]);  flatten_1 = None\n",
      "    contiguous_2: \"f32[1, 4, 2]\" = torch.ops.aten.contiguous.default(permute_3);  permute_3 = None\n",
      "    empty: \"f32[1, 8, 8]\" = torch.ops.aten.empty.memory_format([1, 8, 8], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    zeros_2: \"f32[1]\" = torch.ops.aten.zeros.default([1], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    ones: \"f32[1]\" = torch.ops.aten.ones.default([1], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    select_8: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 0)\n",
      "    select_9: \"f32[1]\" = torch.ops.aten.select.int(select_8, 1, 0);  select_8 = None\n",
      "    select_10: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 0)\n",
      "    select_11: \"f32[1]\" = torch.ops.aten.select.int(select_10, 1, 1);  select_10 = None\n",
      "    select_12: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 0)\n",
      "    select_13: \"f32[1]\" = torch.ops.aten.select.int(select_12, 1, 0);  select_12 = None\n",
      "    select_14: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 0)\n",
      "    select_15: \"f32[1]\" = torch.ops.aten.select.int(select_14, 1, 1);  select_14 = None\n",
      "    neg: \"f32[1]\" = torch.ops.aten.neg.default(select_9)\n",
      "    mul_8: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg, select_13);  neg = None\n",
      "    neg_1: \"f32[1]\" = torch.ops.aten.neg.default(select_11)\n",
      "    mul_9: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_1, select_13);  neg_1 = select_13 = None\n",
      "    stack_3: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_9, select_11, ones, zeros_2, zeros_2, zeros_2, mul_8, mul_9], -1);  mul_8 = mul_9 = None\n",
      "    select_16: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 0)\n",
      "    copy__4: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_16, stack_3);  select_16 = stack_3 = copy__4 = None\n",
      "    neg_2: \"f32[1]\" = torch.ops.aten.neg.default(select_9)\n",
      "    mul_10: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_2, select_15);  neg_2 = None\n",
      "    neg_3: \"f32[1]\" = torch.ops.aten.neg.default(select_11)\n",
      "    mul_11: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_3, select_15);  neg_3 = select_15 = None\n",
      "    stack_4: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_2, zeros_2, zeros_2, select_9, select_11, ones, mul_10, mul_11], -1);  select_9 = select_11 = mul_10 = mul_11 = None\n",
      "    select_17: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 1)\n",
      "    copy__5: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_17, stack_4);  select_17 = stack_4 = copy__5 = None\n",
      "    select_18: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 1)\n",
      "    select_19: \"f32[1]\" = torch.ops.aten.select.int(select_18, 1, 0);  select_18 = None\n",
      "    select_20: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 1)\n",
      "    select_21: \"f32[1]\" = torch.ops.aten.select.int(select_20, 1, 1);  select_20 = None\n",
      "    select_22: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 1)\n",
      "    select_23: \"f32[1]\" = torch.ops.aten.select.int(select_22, 1, 0);  select_22 = None\n",
      "    select_24: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 1)\n",
      "    select_25: \"f32[1]\" = torch.ops.aten.select.int(select_24, 1, 1);  select_24 = None\n",
      "    neg_4: \"f32[1]\" = torch.ops.aten.neg.default(select_19)\n",
      "    mul_12: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_4, select_23);  neg_4 = None\n",
      "    neg_5: \"f32[1]\" = torch.ops.aten.neg.default(select_21)\n",
      "    mul_13: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_5, select_23);  neg_5 = select_23 = None\n",
      "    stack_5: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_19, select_21, ones, zeros_2, zeros_2, zeros_2, mul_12, mul_13], -1);  mul_12 = mul_13 = None\n",
      "    select_26: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 2)\n",
      "    copy__6: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_26, stack_5);  select_26 = stack_5 = copy__6 = None\n",
      "    neg_6: \"f32[1]\" = torch.ops.aten.neg.default(select_19)\n",
      "    mul_14: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_6, select_25);  neg_6 = None\n",
      "    neg_7: \"f32[1]\" = torch.ops.aten.neg.default(select_21)\n",
      "    mul_15: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_7, select_25);  neg_7 = select_25 = None\n",
      "    stack_6: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_2, zeros_2, zeros_2, select_19, select_21, ones, mul_14, mul_15], -1);  select_19 = select_21 = mul_14 = mul_15 = None\n",
      "    select_27: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 3)\n",
      "    copy__7: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_27, stack_6);  select_27 = stack_6 = copy__7 = None\n",
      "    select_28: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 2)\n",
      "    select_29: \"f32[1]\" = torch.ops.aten.select.int(select_28, 1, 0);  select_28 = None\n",
      "    select_30: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 2)\n",
      "    select_31: \"f32[1]\" = torch.ops.aten.select.int(select_30, 1, 1);  select_30 = None\n",
      "    select_32: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 2)\n",
      "    select_33: \"f32[1]\" = torch.ops.aten.select.int(select_32, 1, 0);  select_32 = None\n",
      "    select_34: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 2)\n",
      "    select_35: \"f32[1]\" = torch.ops.aten.select.int(select_34, 1, 1);  select_34 = None\n",
      "    neg_8: \"f32[1]\" = torch.ops.aten.neg.default(select_29)\n",
      "    mul_16: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_8, select_33);  neg_8 = None\n",
      "    neg_9: \"f32[1]\" = torch.ops.aten.neg.default(select_31)\n",
      "    mul_17: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_9, select_33);  neg_9 = select_33 = None\n",
      "    stack_7: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_29, select_31, ones, zeros_2, zeros_2, zeros_2, mul_16, mul_17], -1);  mul_16 = mul_17 = None\n",
      "    select_36: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 4)\n",
      "    copy__8: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_36, stack_7);  select_36 = stack_7 = copy__8 = None\n",
      "    neg_10: \"f32[1]\" = torch.ops.aten.neg.default(select_29)\n",
      "    mul_18: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_10, select_35);  neg_10 = None\n",
      "    neg_11: \"f32[1]\" = torch.ops.aten.neg.default(select_31)\n",
      "    mul_19: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_11, select_35);  neg_11 = select_35 = None\n",
      "    stack_8: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_2, zeros_2, zeros_2, select_29, select_31, ones, mul_18, mul_19], -1);  select_29 = select_31 = mul_18 = mul_19 = None\n",
      "    select_37: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 5)\n",
      "    copy__9: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_37, stack_8);  select_37 = stack_8 = copy__9 = None\n",
      "    select_38: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 3)\n",
      "    select_39: \"f32[1]\" = torch.ops.aten.select.int(select_38, 1, 0);  select_38 = None\n",
      "    select_40: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 3);  contiguous_1 = None\n",
      "    select_41: \"f32[1]\" = torch.ops.aten.select.int(select_40, 1, 1);  select_40 = None\n",
      "    select_42: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 3)\n",
      "    select_43: \"f32[1]\" = torch.ops.aten.select.int(select_42, 1, 0);  select_42 = None\n",
      "    select_44: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 3)\n",
      "    select_45: \"f32[1]\" = torch.ops.aten.select.int(select_44, 1, 1);  select_44 = None\n",
      "    neg_12: \"f32[1]\" = torch.ops.aten.neg.default(select_39)\n",
      "    mul_20: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_12, select_43);  neg_12 = None\n",
      "    neg_13: \"f32[1]\" = torch.ops.aten.neg.default(select_41)\n",
      "    mul_21: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_13, select_43);  neg_13 = select_43 = None\n",
      "    stack_9: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_39, select_41, ones, zeros_2, zeros_2, zeros_2, mul_20, mul_21], -1);  mul_20 = mul_21 = None\n",
      "    select_46: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 6)\n",
      "    copy__10: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_46, stack_9);  select_46 = stack_9 = copy__10 = None\n",
      "    neg_14: \"f32[1]\" = torch.ops.aten.neg.default(select_39)\n",
      "    mul_22: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_14, select_45);  neg_14 = None\n",
      "    neg_15: \"f32[1]\" = torch.ops.aten.neg.default(select_41)\n",
      "    mul_23: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_15, select_45);  neg_15 = select_45 = None\n",
      "    stack_10: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_2, zeros_2, zeros_2, select_39, select_41, ones, mul_22, mul_23], -1);  zeros_2 = select_39 = select_41 = ones = mul_22 = mul_23 = None\n",
      "    select_47: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 7)\n",
      "    copy__11: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_47, stack_10);  select_47 = stack_10 = copy__11 = None\n",
      "    view_11: \"f32[1, 8, 1]\" = torch.ops.aten.view.default(contiguous_2, [-1, 8, 1]);  contiguous_2 = None\n",
      "    to_12: \"f64[1, 8, 8]\" = torch.ops.aten.to.dtype(empty, torch.float64);  empty = None\n",
      "    to_13: \"f64[1, 8, 1]\" = torch.ops.aten.to.dtype(view_11, torch.float64);  view_11 = None\n",
      "    linalg_solve: \"f64[1, 8, 1]\" = torch.ops.aten.linalg_solve.default(to_12, to_13);  to_12 = to_13 = None\n",
      "    to_14: \"f32[1, 8, 1]\" = torch.ops.aten.to.dtype(linalg_solve, torch.float32);  linalg_solve = None\n",
      "    empty_1: \"f32[1, 9]\" = torch.ops.aten.empty.memory_format([1, 9], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    select_48: \"f32[1, 8]\" = torch.ops.aten.select.int(to_14, 2, 0);  to_14 = None\n",
      "    slice_1: \"f32[1, 8]\" = torch.ops.aten.slice.Tensor(empty_1, 1, 0, 8)\n",
      "    copy__12: \"f32[1, 8]\" = torch.ops.aten.copy_.default(slice_1, select_48);  slice_1 = select_48 = copy__12 = None\n",
      "    select_49: \"f32[1]\" = torch.ops.aten.select.int(empty_1, 1, -1)\n",
      "    fill_: \"f32[1]\" = torch.ops.aten.fill_.Scalar(select_49, 1);  select_49 = fill_ = None\n",
      "    view_12: \"f32[1, 3, 3]\" = torch.ops.aten.view.default(empty_1, [-1, 3, 3]);  empty_1 = None\n",
      "    linspace_2: \"f32[64]\" = torch.ops.aten.linspace.default(0, 63, 64, device = device(type='cpu'), pin_memory = False)\n",
      "    linspace_3: \"f32[64]\" = torch.ops.aten.linspace.default(0, 63, 64, device = device(type='cpu'), pin_memory = False)\n",
      "    meshgrid_3 = torch.ops.aten.meshgrid.default([linspace_2, linspace_3]);  linspace_2 = linspace_3 = None\n",
      "    getitem_14: \"f32[64, 64]\" = meshgrid_3[0]\n",
      "    getitem_15: \"f32[64, 64]\" = meshgrid_3[1];  meshgrid_3 = None\n",
      "    flatten_2: \"f32[4096]\" = torch.ops.aten.flatten.using_ints(getitem_15);  getitem_15 = None\n",
      "    unsqueeze_9: \"f32[1, 4096]\" = torch.ops.aten.unsqueeze.default(flatten_2, 0);  flatten_2 = None\n",
      "    flatten_3: \"f32[4096]\" = torch.ops.aten.flatten.using_ints(getitem_14);  getitem_14 = None\n",
      "    unsqueeze_10: \"f32[1, 4096]\" = torch.ops.aten.unsqueeze.default(flatten_3, 0);  flatten_3 = None\n",
      "    ones_1: \"f32[1, 4096]\" = torch.ops.aten.ones.default([1, 4096], device = device(type='cpu'), pin_memory = False)\n",
      "    cat_6: \"f32[3, 4096]\" = torch.ops.aten.cat.default([unsqueeze_9, unsqueeze_10, ones_1]);  unsqueeze_9 = unsqueeze_10 = ones_1 = None\n",
      "    unsqueeze_11: \"f32[1, 3, 4096]\" = torch.ops.aten.unsqueeze.default(cat_6, 0);  cat_6 = None\n",
      "    repeat_1: \"f32[1, 3, 4096]\" = torch.ops.aten.repeat.default(unsqueeze_11, [1, 1, 1]);  unsqueeze_11 = None\n",
      "    to_15: \"f32[1, 3, 4096]\" = torch.ops.aten.to.dtype_layout(repeat_1, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  repeat_1 = None\n",
      "    bmm: \"f32[1, 3, 4096]\" = torch.ops.aten.bmm.default(view_12, to_15);  view_12 = to_15 = None\n",
      "    isnan: \"b8[1, 3, 4096]\" = torch.ops.aten.isnan.default(bmm);  bmm = None\n",
      "    any_1: \"b8[]\" = torch.ops.aten.any.default(isnan);  isnan = None\n",
      "    ne: \"b8[]\" = torch.ops.aten.ne.Scalar(any_1, 0);  any_1 = None\n",
      "    item: \"Sym(Eq(u0, 1))\" = torch.ops.aten.item.default(ne);  ne = item = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:149 in forward, code: coords1 = self.get_flow_now_4(four_point_disp)\n",
      "    div_15: \"f32[1, 2, 2, 2]\" = torch.ops.aten.div.Tensor(to_9, 4);  to_9 = None\n",
      "    zeros_3: \"f32[2, 2, 2]\" = torch.ops.aten.zeros.default([2, 2, 2], device = device(type='cpu'), pin_memory = False)\n",
      "    to_16: \"f32[2, 2, 2]\" = torch.ops.aten.to.dtype_layout(zeros_3, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  zeros_3 = None\n",
      "    _tensor_constant6: \"f32[2]\" = self._tensor_constant6\n",
      "    lift_fresh_copy_6: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None\n",
      "    select_50: \"f32[2, 2]\" = torch.ops.aten.select.int(to_16, 1, 0)\n",
      "    select_51: \"f32[2]\" = torch.ops.aten.select.int(select_50, 1, 0);  select_50 = None\n",
      "    copy__13: \"f32[2]\" = torch.ops.aten.copy_.default(select_51, lift_fresh_copy_6);  select_51 = lift_fresh_copy_6 = copy__13 = None\n",
      "    _tensor_constant7: \"f32[2]\" = self._tensor_constant7\n",
      "    lift_fresh_copy_7: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant7);  _tensor_constant7 = None\n",
      "    select_52: \"f32[2, 2]\" = torch.ops.aten.select.int(to_16, 1, 0)\n",
      "    select_53: \"f32[2]\" = torch.ops.aten.select.int(select_52, 1, 1);  select_52 = None\n",
      "    copy__14: \"f32[2]\" = torch.ops.aten.copy_.default(select_53, lift_fresh_copy_7);  select_53 = lift_fresh_copy_7 = copy__14 = None\n",
      "    _tensor_constant8: \"f32[2]\" = self._tensor_constant8\n",
      "    lift_fresh_copy_8: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant8);  _tensor_constant8 = None\n",
      "    select_54: \"f32[2, 2]\" = torch.ops.aten.select.int(to_16, 1, 1)\n",
      "    select_55: \"f32[2]\" = torch.ops.aten.select.int(select_54, 1, 0);  select_54 = None\n",
      "    copy__15: \"f32[2]\" = torch.ops.aten.copy_.default(select_55, lift_fresh_copy_8);  select_55 = lift_fresh_copy_8 = copy__15 = None\n",
      "    _tensor_constant9: \"f32[2]\" = self._tensor_constant9\n",
      "    lift_fresh_copy_9: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant9);  _tensor_constant9 = None\n",
      "    select_56: \"f32[2, 2]\" = torch.ops.aten.select.int(to_16, 1, 1)\n",
      "    select_57: \"f32[2]\" = torch.ops.aten.select.int(select_56, 1, 1);  select_56 = None\n",
      "    copy__16: \"f32[2]\" = torch.ops.aten.copy_.default(select_57, lift_fresh_copy_9);  select_57 = lift_fresh_copy_9 = copy__16 = None\n",
      "    unsqueeze_12: \"f32[1, 2, 2, 2]\" = torch.ops.aten.unsqueeze.default(to_16, 0);  to_16 = None\n",
      "    repeat_2: \"f32[1, 2, 2, 2]\" = torch.ops.aten.repeat.default(unsqueeze_12, [1, 1, 1, 1]);  unsqueeze_12 = None\n",
      "    add_14: \"f32[1, 2, 2, 2]\" = torch.ops.aten.add.Tensor(repeat_2, div_15);  div_15 = None\n",
      "    flatten_4: \"f32[1, 2, 4]\" = torch.ops.aten.flatten.using_ints(repeat_2, 2);  repeat_2 = None\n",
      "    permute_4: \"f32[1, 4, 2]\" = torch.ops.aten.permute.default(flatten_4, [0, 2, 1]);  flatten_4 = None\n",
      "    contiguous_3: \"f32[1, 4, 2]\" = torch.ops.aten.contiguous.default(permute_4);  permute_4 = None\n",
      "    flatten_5: \"f32[1, 2, 4]\" = torch.ops.aten.flatten.using_ints(add_14, 2);  add_14 = None\n",
      "    permute_5: \"f32[1, 4, 2]\" = torch.ops.aten.permute.default(flatten_5, [0, 2, 1]);  flatten_5 = None\n",
      "    contiguous_4: \"f32[1, 4, 2]\" = torch.ops.aten.contiguous.default(permute_5);  permute_5 = None\n",
      "    empty_2: \"f32[1, 8, 8]\" = torch.ops.aten.empty.memory_format([1, 8, 8], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    zeros_4: \"f32[1]\" = torch.ops.aten.zeros.default([1], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    ones_2: \"f32[1]\" = torch.ops.aten.ones.default([1], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    select_58: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 0)\n",
      "    select_59: \"f32[1]\" = torch.ops.aten.select.int(select_58, 1, 0);  select_58 = None\n",
      "    select_60: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 0)\n",
      "    select_61: \"f32[1]\" = torch.ops.aten.select.int(select_60, 1, 1);  select_60 = None\n",
      "    select_62: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 0)\n",
      "    select_63: \"f32[1]\" = torch.ops.aten.select.int(select_62, 1, 0);  select_62 = None\n",
      "    select_64: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 0)\n",
      "    select_65: \"f32[1]\" = torch.ops.aten.select.int(select_64, 1, 1);  select_64 = None\n",
      "    neg_16: \"f32[1]\" = torch.ops.aten.neg.default(select_59)\n",
      "    mul_24: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_16, select_63);  neg_16 = None\n",
      "    neg_17: \"f32[1]\" = torch.ops.aten.neg.default(select_61)\n",
      "    mul_25: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_17, select_63);  neg_17 = select_63 = None\n",
      "    stack_11: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_59, select_61, ones_2, zeros_4, zeros_4, zeros_4, mul_24, mul_25], -1);  mul_24 = mul_25 = None\n",
      "    select_66: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 0)\n",
      "    copy__17: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_66, stack_11);  select_66 = stack_11 = copy__17 = None\n",
      "    neg_18: \"f32[1]\" = torch.ops.aten.neg.default(select_59)\n",
      "    mul_26: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_18, select_65);  neg_18 = None\n",
      "    neg_19: \"f32[1]\" = torch.ops.aten.neg.default(select_61)\n",
      "    mul_27: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_19, select_65);  neg_19 = select_65 = None\n",
      "    stack_12: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_4, zeros_4, zeros_4, select_59, select_61, ones_2, mul_26, mul_27], -1);  select_59 = select_61 = mul_26 = mul_27 = None\n",
      "    select_67: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 1)\n",
      "    copy__18: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_67, stack_12);  select_67 = stack_12 = copy__18 = None\n",
      "    select_68: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 1)\n",
      "    select_69: \"f32[1]\" = torch.ops.aten.select.int(select_68, 1, 0);  select_68 = None\n",
      "    select_70: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 1)\n",
      "    select_71: \"f32[1]\" = torch.ops.aten.select.int(select_70, 1, 1);  select_70 = None\n",
      "    select_72: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 1)\n",
      "    select_73: \"f32[1]\" = torch.ops.aten.select.int(select_72, 1, 0);  select_72 = None\n",
      "    select_74: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 1)\n",
      "    select_75: \"f32[1]\" = torch.ops.aten.select.int(select_74, 1, 1);  select_74 = None\n",
      "    neg_20: \"f32[1]\" = torch.ops.aten.neg.default(select_69)\n",
      "    mul_28: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_20, select_73);  neg_20 = None\n",
      "    neg_21: \"f32[1]\" = torch.ops.aten.neg.default(select_71)\n",
      "    mul_29: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_21, select_73);  neg_21 = select_73 = None\n",
      "    stack_13: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_69, select_71, ones_2, zeros_4, zeros_4, zeros_4, mul_28, mul_29], -1);  mul_28 = mul_29 = None\n",
      "    select_76: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 2)\n",
      "    copy__19: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_76, stack_13);  select_76 = stack_13 = copy__19 = None\n",
      "    neg_22: \"f32[1]\" = torch.ops.aten.neg.default(select_69)\n",
      "    mul_30: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_22, select_75);  neg_22 = None\n",
      "    neg_23: \"f32[1]\" = torch.ops.aten.neg.default(select_71)\n",
      "    mul_31: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_23, select_75);  neg_23 = select_75 = None\n",
      "    stack_14: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_4, zeros_4, zeros_4, select_69, select_71, ones_2, mul_30, mul_31], -1);  select_69 = select_71 = mul_30 = mul_31 = None\n",
      "    select_77: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 3)\n",
      "    copy__20: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_77, stack_14);  select_77 = stack_14 = copy__20 = None\n",
      "    select_78: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 2)\n",
      "    select_79: \"f32[1]\" = torch.ops.aten.select.int(select_78, 1, 0);  select_78 = None\n",
      "    select_80: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 2)\n",
      "    select_81: \"f32[1]\" = torch.ops.aten.select.int(select_80, 1, 1);  select_80 = None\n",
      "    select_82: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 2)\n",
      "    select_83: \"f32[1]\" = torch.ops.aten.select.int(select_82, 1, 0);  select_82 = None\n",
      "    select_84: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 2)\n",
      "    select_85: \"f32[1]\" = torch.ops.aten.select.int(select_84, 1, 1);  select_84 = None\n",
      "    neg_24: \"f32[1]\" = torch.ops.aten.neg.default(select_79)\n",
      "    mul_32: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_24, select_83);  neg_24 = None\n",
      "    neg_25: \"f32[1]\" = torch.ops.aten.neg.default(select_81)\n",
      "    mul_33: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_25, select_83);  neg_25 = select_83 = None\n",
      "    stack_15: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_79, select_81, ones_2, zeros_4, zeros_4, zeros_4, mul_32, mul_33], -1);  mul_32 = mul_33 = None\n",
      "    select_86: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 4)\n",
      "    copy__21: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_86, stack_15);  select_86 = stack_15 = copy__21 = None\n",
      "    neg_26: \"f32[1]\" = torch.ops.aten.neg.default(select_79)\n",
      "    mul_34: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_26, select_85);  neg_26 = None\n",
      "    neg_27: \"f32[1]\" = torch.ops.aten.neg.default(select_81)\n",
      "    mul_35: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_27, select_85);  neg_27 = select_85 = None\n",
      "    stack_16: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_4, zeros_4, zeros_4, select_79, select_81, ones_2, mul_34, mul_35], -1);  select_79 = select_81 = mul_34 = mul_35 = None\n",
      "    select_87: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 5)\n",
      "    copy__22: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_87, stack_16);  select_87 = stack_16 = copy__22 = None\n",
      "    select_88: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 3)\n",
      "    select_89: \"f32[1]\" = torch.ops.aten.select.int(select_88, 1, 0);  select_88 = None\n",
      "    select_90: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 3);  contiguous_3 = None\n",
      "    select_91: \"f32[1]\" = torch.ops.aten.select.int(select_90, 1, 1);  select_90 = None\n",
      "    select_92: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 3)\n",
      "    select_93: \"f32[1]\" = torch.ops.aten.select.int(select_92, 1, 0);  select_92 = None\n",
      "    select_94: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 3)\n",
      "    select_95: \"f32[1]\" = torch.ops.aten.select.int(select_94, 1, 1);  select_94 = None\n",
      "    neg_28: \"f32[1]\" = torch.ops.aten.neg.default(select_89)\n",
      "    mul_36: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_28, select_93);  neg_28 = None\n",
      "    neg_29: \"f32[1]\" = torch.ops.aten.neg.default(select_91)\n",
      "    mul_37: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_29, select_93);  neg_29 = select_93 = None\n",
      "    stack_17: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_89, select_91, ones_2, zeros_4, zeros_4, zeros_4, mul_36, mul_37], -1);  mul_36 = mul_37 = None\n",
      "    select_96: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 6)\n",
      "    copy__23: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_96, stack_17);  select_96 = stack_17 = copy__23 = None\n",
      "    neg_30: \"f32[1]\" = torch.ops.aten.neg.default(select_89)\n",
      "    mul_38: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_30, select_95);  neg_30 = None\n",
      "    neg_31: \"f32[1]\" = torch.ops.aten.neg.default(select_91)\n",
      "    mul_39: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_31, select_95);  neg_31 = select_95 = None\n",
      "    stack_18: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_4, zeros_4, zeros_4, select_89, select_91, ones_2, mul_38, mul_39], -1);  zeros_4 = select_89 = select_91 = ones_2 = mul_38 = mul_39 = None\n",
      "    select_97: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 7)\n",
      "    copy__24: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_97, stack_18);  select_97 = stack_18 = copy__24 = None\n",
      "    view_13: \"f32[1, 8, 1]\" = torch.ops.aten.view.default(contiguous_4, [-1, 8, 1]);  contiguous_4 = None\n",
      "    to_17: \"f64[1, 8, 8]\" = torch.ops.aten.to.dtype(empty_2, torch.float64);  empty_2 = None\n",
      "    to_18: \"f64[1, 8, 1]\" = torch.ops.aten.to.dtype(view_13, torch.float64);  view_13 = None\n",
      "    linalg_solve_1: \"f64[1, 8, 1]\" = torch.ops.aten.linalg_solve.default(to_17, to_18);  to_17 = to_18 = None\n",
      "    to_19: \"f32[1, 8, 1]\" = torch.ops.aten.to.dtype(linalg_solve_1, torch.float32);  linalg_solve_1 = None\n",
      "    empty_3: \"f32[1, 9]\" = torch.ops.aten.empty.memory_format([1, 9], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    select_98: \"f32[1, 8]\" = torch.ops.aten.select.int(to_19, 2, 0);  to_19 = None\n",
      "    slice_2: \"f32[1, 8]\" = torch.ops.aten.slice.Tensor(empty_3, 1, 0, 8)\n",
      "    copy__25: \"f32[1, 8]\" = torch.ops.aten.copy_.default(slice_2, select_98);  slice_2 = select_98 = copy__25 = None\n",
      "    select_99: \"f32[1]\" = torch.ops.aten.select.int(empty_3, 1, -1)\n",
      "    fill__1: \"f32[1]\" = torch.ops.aten.fill_.Scalar(select_99, 1);  select_99 = fill__1 = None\n",
      "    view_14: \"f32[1, 3, 3]\" = torch.ops.aten.view.default(empty_3, [-1, 3, 3]);  empty_3 = None\n",
      "    linspace_4: \"f32[64]\" = torch.ops.aten.linspace.default(0, 63, 64, device = device(type='cpu'), pin_memory = False)\n",
      "    linspace_5: \"f32[64]\" = torch.ops.aten.linspace.default(0, 63, 64, device = device(type='cpu'), pin_memory = False)\n",
      "    meshgrid_4 = torch.ops.aten.meshgrid.default([linspace_4, linspace_5]);  linspace_4 = linspace_5 = None\n",
      "    getitem_16: \"f32[64, 64]\" = meshgrid_4[0]\n",
      "    getitem_17: \"f32[64, 64]\" = meshgrid_4[1];  meshgrid_4 = None\n",
      "    flatten_6: \"f32[4096]\" = torch.ops.aten.flatten.using_ints(getitem_17);  getitem_17 = None\n",
      "    unsqueeze_13: \"f32[1, 4096]\" = torch.ops.aten.unsqueeze.default(flatten_6, 0);  flatten_6 = None\n",
      "    flatten_7: \"f32[4096]\" = torch.ops.aten.flatten.using_ints(getitem_16);  getitem_16 = None\n",
      "    unsqueeze_14: \"f32[1, 4096]\" = torch.ops.aten.unsqueeze.default(flatten_7, 0);  flatten_7 = None\n",
      "    ones_3: \"f32[1, 4096]\" = torch.ops.aten.ones.default([1, 4096], device = device(type='cpu'), pin_memory = False)\n",
      "    cat_7: \"f32[3, 4096]\" = torch.ops.aten.cat.default([unsqueeze_13, unsqueeze_14, ones_3]);  unsqueeze_13 = unsqueeze_14 = ones_3 = None\n",
      "    unsqueeze_15: \"f32[1, 3, 4096]\" = torch.ops.aten.unsqueeze.default(cat_7, 0);  cat_7 = None\n",
      "    repeat_3: \"f32[1, 3, 4096]\" = torch.ops.aten.repeat.default(unsqueeze_15, [1, 1, 1]);  unsqueeze_15 = None\n",
      "    to_20: \"f32[1, 3, 4096]\" = torch.ops.aten.to.dtype_layout(repeat_3, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  repeat_3 = None\n",
      "    bmm_1: \"f32[1, 3, 4096]\" = torch.ops.aten.bmm.default(view_14, to_20);  view_14 = to_20 = None\n",
      "    isnan_1: \"b8[1, 3, 4096]\" = torch.ops.aten.isnan.default(bmm_1);  bmm_1 = None\n",
      "    any_2: \"b8[]\" = torch.ops.aten.any.default(isnan_1);  isnan_1 = None\n",
      "    ne_1: \"b8[]\" = torch.ops.aten.ne.Scalar(any_2, 0);  any_2 = None\n",
      "    item_1: \"Sym(Eq(u1, 1))\" = torch.ops.aten.item.default(ne_1);  ne_1 = item_1 = None\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1: \"f32[64, 3, 7, 7]\", arg1_1: \"f32[64]\", arg2_1: \"f32[64, 64, 3, 3]\", arg3_1: \"f32[64]\", arg4_1: \"f32[64, 64, 3, 3]\", arg5_1: \"f32[64]\", arg6_1: \"f32[64, 64, 1, 1]\", arg7_1: \"f32[64]\", arg8_1: \"f32[64, 64, 3, 3]\", arg9_1: \"f32[64]\", arg10_1: \"f32[64, 64, 3, 3]\", arg11_1: \"f32[64]\", arg12_1: \"f32[64, 64, 1, 1]\", arg13_1: \"f32[64]\", arg14_1: \"f32[96, 64, 3, 3]\", arg15_1: \"f32[96]\", arg16_1: \"f32[96, 96, 3, 3]\", arg17_1: \"f32[96]\", arg18_1: \"f32[96, 64, 1, 1]\", arg19_1: \"f32[96]\", arg20_1: \"f32[96, 96, 3, 3]\", arg21_1: \"f32[96]\", arg22_1: \"f32[96, 96, 3, 3]\", arg23_1: \"f32[96]\", arg24_1: \"f32[96, 96, 1, 1]\", arg25_1: \"f32[96]\", arg26_1: \"f32[256, 96, 1, 1]\", arg27_1: \"f32[256]\", arg28_1: \"f32[128, 326, 3, 3]\", arg29_1: \"f32[128]\", arg30_1: \"f32[128]\", arg31_1: \"f32[128]\", arg32_1: \"f32[128, 128, 3, 3]\", arg33_1: \"f32[128]\", arg34_1: \"f32[128]\", arg35_1: \"f32[128]\", arg36_1: \"f32[128, 128, 3, 3]\", arg37_1: \"f32[128]\", arg38_1: \"f32[128]\", arg39_1: \"f32[128]\", arg40_1: \"f32[128, 128, 3, 3]\", arg41_1: \"f32[128]\", arg42_1: \"f32[128]\", arg43_1: \"f32[128]\", arg44_1: \"f32[128, 128, 3, 3]\", arg45_1: \"f32[128]\", arg46_1: \"f32[128]\", arg47_1: \"f32[128]\", arg48_1: \"f32[128, 128, 3, 3]\", arg49_1: \"f32[128]\", arg50_1: \"f32[128]\", arg51_1: \"f32[128]\", arg52_1: \"f32[2, 128, 1, 1]\", arg53_1: \"f32[2]\", arg54_1: \"f32[1, 3, 256, 256]\", arg55_1: \"f32[1, 3, 256, 256]\"):\n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:108 in forward, code: self.imagenet_mean = torch.Tensor([0.485, 0.456, 0.406]).unsqueeze(0).unsqueeze(2).unsqueeze(3).to(image1.device)\n",
      "    _tensor_constant0: \"f32[3]\" = self._tensor_constant0\n",
      "    lift_fresh_copy: \"f32[3]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
      "    unsqueeze: \"f32[1, 3]\" = torch.ops.aten.unsqueeze.default(lift_fresh_copy, 0);  lift_fresh_copy = None\n",
      "    unsqueeze_1: \"f32[1, 3, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None\n",
      "    unsqueeze_2: \"f32[1, 3, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_1, 3);  unsqueeze_1 = None\n",
      "    to: \"f32[1, 3, 1, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_2, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  unsqueeze_2 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:109 in forward, code: self.imagenet_std = torch.Tensor([0.229, 0.224, 0.225]).unsqueeze(0).unsqueeze(2).unsqueeze(3).to(image1.device)\n",
      "    _tensor_constant1: \"f32[3]\" = self._tensor_constant1\n",
      "    lift_fresh_copy_1: \"f32[3]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None\n",
      "    unsqueeze_3: \"f32[1, 3]\" = torch.ops.aten.unsqueeze.default(lift_fresh_copy_1, 0);  lift_fresh_copy_1 = None\n",
      "    unsqueeze_4: \"f32[1, 3, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None\n",
      "    unsqueeze_5: \"f32[1, 3, 1, 1]\" = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = None\n",
      "    to_1: \"f32[1, 3, 1, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_5, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  unsqueeze_5 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:110 in forward, code: image1 = (image1.contiguous() - self.imagenet_mean) / self.imagenet_std\n",
      "    sub: \"f32[1, 3, 256, 256]\" = torch.ops.aten.sub.Tensor(arg54_1, to);  arg54_1 = None\n",
      "    div: \"f32[1, 3, 256, 256]\" = torch.ops.aten.div.Tensor(sub, to_1);  sub = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:111 in forward, code: image2 = (image2.contiguous() - self.imagenet_mean) / self.imagenet_std\n",
      "    sub_1: \"f32[1, 3, 256, 256]\" = torch.ops.aten.sub.Tensor(arg55_1, to);  arg55_1 = to = None\n",
      "    div_1: \"f32[1, 3, 256, 256]\" = torch.ops.aten.div.Tensor(sub_1, to_1);  sub_1 = to_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:113 in forward, code: with _autocast(self.device.type, enabled=self.args.mixed_precision and self.device.type != \"cpu\"):\n",
      "    _enter_autocast = torch.amp.autocast_mode._enter_autocast('cpu', torch.bfloat16, False, False)\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d: \"f32[1, 64, 256, 256]\" = torch.ops.aten.conv2d.default(div, arg0_1, arg1_1, [1, 1], [3, 3]);  div = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm: \"f32[1, 64, 256, 256]\" = torch.ops.aten.instance_norm.default(conv2d, None, None, None, None, True, 0.1, 1e-05, True);  conv2d = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_: \"f32[1, 64, 256, 256]\" = torch.ops.aten.relu_.default(instance_norm);  instance_norm = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:255 in forward, code: x = F.max_pool2d(x, 2, stride=2)\n",
      "    max_pool2d: \"f32[1, 64, 128, 128]\" = torch.ops.aten.max_pool2d.default(relu_, [2, 2], [2, 2]);  relu_ = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_1: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(max_pool2d, arg2_1, arg3_1, [1, 1], [1, 1])\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_1: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_1, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__1: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_1);  instance_norm_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_2: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__1, arg4_1, arg5_1, [1, 1], [1, 1]);  relu__1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_2: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_2, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__2: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_2);  instance_norm_2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_3: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(max_pool2d, arg6_1, arg7_1);  max_pool2d = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_3: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_3, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_3 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add: \"f32[1, 64, 128, 128]\" = torch.ops.aten.add.Tensor(instance_norm_3, relu__2);  instance_norm_3 = relu__2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__3: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(add);  add = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_4: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__3, arg8_1, arg9_1, [1, 1], [1, 1])\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_4: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_4, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__4: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_4);  instance_norm_4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_5: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__4, arg10_1, arg11_1, [1, 1], [1, 1]);  relu__4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_5: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_5, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_5 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__5: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_5);  instance_norm_5 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_6: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__3, arg12_1, arg13_1);  relu__3 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_6: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_6, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_6 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_1: \"f32[1, 64, 128, 128]\" = torch.ops.aten.add.Tensor(instance_norm_6, relu__5);  instance_norm_6 = relu__5 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__6: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(add_1);  add_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:257 in forward, code: x = F.max_pool2d(x, 2, stride=2)\n",
      "    max_pool2d_1: \"f32[1, 64, 64, 64]\" = torch.ops.aten.max_pool2d.default(relu__6, [2, 2], [2, 2]);  relu__6 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_7: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(max_pool2d_1, arg14_1, arg15_1, [1, 1], [1, 1])\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_7: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_7, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_7 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__7: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_7);  instance_norm_7 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_8: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__7, arg16_1, arg17_1, [1, 1], [1, 1]);  relu__7 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_8: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_8, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_8 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__8: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_8);  instance_norm_8 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_9: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(max_pool2d_1, arg18_1, arg19_1);  max_pool2d_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_9: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_9, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_9 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_2: \"f32[1, 96, 64, 64]\" = torch.ops.aten.add.Tensor(instance_norm_9, relu__8);  instance_norm_9 = relu__8 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__9: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(add_2);  add_2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_10: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__9, arg20_1, arg21_1, [1, 1], [1, 1])\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_10: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_10, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_10 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__10: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_10);  instance_norm_10 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_11: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__10, arg22_1, arg23_1, [1, 1], [1, 1]);  relu__10 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_11: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_11, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_11 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__11: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_11);  instance_norm_11 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_12: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__9, arg24_1, arg25_1);  relu__9 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_12: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_12, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_12 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_3: \"f32[1, 96, 64, 64]\" = torch.ops.aten.add.Tensor(instance_norm_12, relu__11);  instance_norm_12 = relu__11 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__12: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(add_3);  add_3 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_13: \"f32[1, 256, 64, 64]\" = torch.ops.aten.conv2d.default(relu__12, arg26_1, arg27_1);  relu__12 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_14: \"f32[1, 64, 256, 256]\" = torch.ops.aten.conv2d.default(div_1, arg0_1, arg1_1, [1, 1], [3, 3]);  div_1 = arg0_1 = arg1_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_13: \"f32[1, 64, 256, 256]\" = torch.ops.aten.instance_norm.default(conv2d_14, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_14 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__13: \"f32[1, 64, 256, 256]\" = torch.ops.aten.relu_.default(instance_norm_13);  instance_norm_13 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:255 in forward, code: x = F.max_pool2d(x, 2, stride=2)\n",
      "    max_pool2d_2: \"f32[1, 64, 128, 128]\" = torch.ops.aten.max_pool2d.default(relu__13, [2, 2], [2, 2]);  relu__13 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_15: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(max_pool2d_2, arg2_1, arg3_1, [1, 1], [1, 1]);  arg2_1 = arg3_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_14: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_15, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_15 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__14: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_14);  instance_norm_14 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_16: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__14, arg4_1, arg5_1, [1, 1], [1, 1]);  relu__14 = arg4_1 = arg5_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_15: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_16, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_16 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__15: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_15);  instance_norm_15 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_17: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(max_pool2d_2, arg6_1, arg7_1);  max_pool2d_2 = arg6_1 = arg7_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_16: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_17, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_17 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_4: \"f32[1, 64, 128, 128]\" = torch.ops.aten.add.Tensor(instance_norm_16, relu__15);  instance_norm_16 = relu__15 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__16: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(add_4);  add_4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_18: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__16, arg8_1, arg9_1, [1, 1], [1, 1]);  arg8_1 = arg9_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_17: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_18, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_18 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__17: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_17);  instance_norm_17 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_19: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__17, arg10_1, arg11_1, [1, 1], [1, 1]);  relu__17 = arg10_1 = arg11_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_18: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_19, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_19 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__18: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(instance_norm_18);  instance_norm_18 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_20: \"f32[1, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu__16, arg12_1, arg13_1);  relu__16 = arg12_1 = arg13_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_19: \"f32[1, 64, 128, 128]\" = torch.ops.aten.instance_norm.default(conv2d_20, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_20 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_5: \"f32[1, 64, 128, 128]\" = torch.ops.aten.add.Tensor(instance_norm_19, relu__18);  instance_norm_19 = relu__18 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__19: \"f32[1, 64, 128, 128]\" = torch.ops.aten.relu_.default(add_5);  add_5 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:257 in forward, code: x = F.max_pool2d(x, 2, stride=2)\n",
      "    max_pool2d_3: \"f32[1, 64, 64, 64]\" = torch.ops.aten.max_pool2d.default(relu__19, [2, 2], [2, 2]);  relu__19 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_21: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(max_pool2d_3, arg14_1, arg15_1, [1, 1], [1, 1]);  arg14_1 = arg15_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_20: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_21, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_21 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__20: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_20);  instance_norm_20 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_22: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__20, arg16_1, arg17_1, [1, 1], [1, 1]);  relu__20 = arg16_1 = arg17_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_21: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_22, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_22 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__21: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_21);  instance_norm_21 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_23: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(max_pool2d_3, arg18_1, arg19_1);  max_pool2d_3 = arg18_1 = arg19_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_22: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_23, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_23 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_6: \"f32[1, 96, 64, 64]\" = torch.ops.aten.add.Tensor(instance_norm_22, relu__21);  instance_norm_22 = relu__21 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__22: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(add_6);  add_6 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_24: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__22, arg20_1, arg21_1, [1, 1], [1, 1]);  arg20_1 = arg21_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_23: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_24, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_24 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__23: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_23);  instance_norm_23 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_25: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__23, arg22_1, arg23_1, [1, 1], [1, 1]);  relu__23 = arg22_1 = arg23_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_24: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_25, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_25 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__24: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(instance_norm_24);  instance_norm_24 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_26: \"f32[1, 96, 64, 64]\" = torch.ops.aten.conv2d.default(relu__22, arg24_1, arg25_1);  relu__22 = arg24_1 = arg25_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:124 in forward, code: return self._apply_instance_norm(input)\n",
      "    instance_norm_25: \"f32[1, 96, 64, 64]\" = torch.ops.aten.instance_norm.default(conv2d_26, None, None, None, None, True, 0.1, 1e-05, True);  conv2d_26 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\extractor.py:46 in forward, code: return self.relu(x + y)\n",
      "    add_7: \"f32[1, 96, 64, 64]\" = torch.ops.aten.add.Tensor(instance_norm_25, relu__24);  instance_norm_25 = relu__24 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu__25: \"f32[1, 96, 64, 64]\" = torch.ops.aten.relu_.default(add_7);  add_7 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_27: \"f32[1, 256, 64, 64]\" = torch.ops.aten.conv2d.default(relu__25, arg26_1, arg27_1);  relu__25 = arg26_1 = arg27_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:113 in forward, code: with _autocast(self.device.type, enabled=self.args.mixed_precision and self.device.type != \"cpu\"):\n",
      "    _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:122 in forward, code: fmap1 = fmap1_64.float()\n",
      "    to_2: \"f32[1, 256, 64, 64]\" = torch.ops.aten.to.dtype(conv2d_13, torch.float32);  conv2d_13 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:123 in forward, code: fmap2 = fmap2_64.float()\n",
      "    to_3: \"f32[1, 256, 64, 64]\" = torch.ops.aten.to.dtype(conv2d_27, torch.float32);  conv2d_27 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:125 in forward, code: corr_fn = CorrBlock(fmap1, fmap2, num_levels=corr_level, radius=corr_radius)\n",
      "    view: \"f32[1, 256, 4096]\" = torch.ops.aten.view.default(to_2, [1, 256, 4096]);  to_2 = None\n",
      "    view_1: \"f32[1, 256, 4096]\" = torch.ops.aten.view.default(to_3, [1, 256, 4096]);  to_3 = None\n",
      "    transpose: \"f32[1, 4096, 256]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
      "    matmul: \"f32[1, 4096, 4096]\" = torch.ops.aten.matmul.default(transpose, view_1);  transpose = view_1 = None\n",
      "    relu: \"f32[1, 4096, 4096]\" = torch.ops.aten.relu.default(matmul);  matmul = None\n",
      "    view_2: \"f32[1, 64, 64, 1, 64, 64]\" = torch.ops.aten.view.default(relu, [1, 64, 64, 1, 64, 64]);  relu = None\n",
      "    reshape: \"f32[4096, 1, 64, 64]\" = torch.ops.aten.reshape.default(view_2, [4096, 1, 64, 64]);  view_2 = None\n",
      "    avg_pool2d: \"f32[4096, 1, 32, 32]\" = torch.ops.aten.avg_pool2d.default(reshape, [2, 2], [2, 2])\n",
      "    avg_pool2d_1: \"f32[4096, 1, 16, 16]\" = torch.ops.aten.avg_pool2d.default(avg_pool2d, [2, 2], [2, 2])\n",
      "    avg_pool2d_2: \"f32[4096, 1, 8, 8]\" = torch.ops.aten.avg_pool2d.default(avg_pool2d_1, [2, 2], [2, 2])\n",
      "    linspace: \"f32[9]\" = torch.ops.aten.linspace.default(-4, 4, 9, device = device(type='cpu'), pin_memory = False)\n",
      "    linspace_1: \"f32[9]\" = torch.ops.aten.linspace.default(-4, 4, 9, device = device(type='cpu'), pin_memory = False)\n",
      "    meshgrid = torch.ops.aten.meshgrid.default([linspace_1, linspace]);  linspace_1 = linspace = None\n",
      "    getitem: \"f32[9, 9]\" = meshgrid[0]\n",
      "    getitem_1: \"f32[9, 9]\" = meshgrid[1];  meshgrid = None\n",
      "    stack: \"f32[9, 9, 2]\" = torch.ops.aten.stack.default([getitem, getitem_1], -1);  getitem = getitem_1 = None\n",
      "    to_4: \"f32[9, 9, 2]\" = torch.ops.aten.to.dtype_layout(stack, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  stack = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:126 in forward, code: coords0, coords1 = self.initialize_flow_4(image1)\n",
      "    arange: \"i64[64]\" = torch.ops.aten.arange.default(64, device = device(type='cpu'), pin_memory = False)\n",
      "    arange_1: \"i64[64]\" = torch.ops.aten.arange.default(64, device = device(type='cpu'), pin_memory = False)\n",
      "    meshgrid_1 = torch.ops.aten.meshgrid.default([arange, arange_1]);  arange = arange_1 = None\n",
      "    getitem_2: \"i64[64, 64]\" = meshgrid_1[0]\n",
      "    getitem_3: \"i64[64, 64]\" = meshgrid_1[1];  meshgrid_1 = None\n",
      "    stack_1: \"i64[2, 64, 64]\" = torch.ops.aten.stack.default([getitem_3, getitem_2]);  getitem_3 = getitem_2 = None\n",
      "    to_5: \"f32[2, 64, 64]\" = torch.ops.aten.to.dtype(stack_1, torch.float32);  stack_1 = None\n",
      "    unsqueeze_6: \"f32[1, 2, 64, 64]\" = torch.ops.aten.unsqueeze.default(to_5, 0);  to_5 = None\n",
      "    expand: \"f32[1, 2, 64, 64]\" = torch.ops.aten.expand.default(unsqueeze_6, [1, -1, -1, -1]);  unsqueeze_6 = None\n",
      "    to_6: \"f32[1, 2, 64, 64]\" = torch.ops.aten.to.dtype_layout(expand, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  expand = None\n",
      "    arange_2: \"i64[64]\" = torch.ops.aten.arange.default(64, device = device(type='cpu'), pin_memory = False)\n",
      "    arange_3: \"i64[64]\" = torch.ops.aten.arange.default(64, device = device(type='cpu'), pin_memory = False)\n",
      "    meshgrid_2 = torch.ops.aten.meshgrid.default([arange_2, arange_3]);  arange_2 = arange_3 = None\n",
      "    getitem_4: \"i64[64, 64]\" = meshgrid_2[0]\n",
      "    getitem_5: \"i64[64, 64]\" = meshgrid_2[1];  meshgrid_2 = None\n",
      "    stack_2: \"i64[2, 64, 64]\" = torch.ops.aten.stack.default([getitem_5, getitem_4]);  getitem_5 = getitem_4 = None\n",
      "    to_7: \"f32[2, 64, 64]\" = torch.ops.aten.to.dtype(stack_2, torch.float32);  stack_2 = None\n",
      "    unsqueeze_7: \"f32[1, 2, 64, 64]\" = torch.ops.aten.unsqueeze.default(to_7, 0);  to_7 = None\n",
      "    expand_1: \"f32[1, 2, 64, 64]\" = torch.ops.aten.expand.default(unsqueeze_7, [1, -1, -1, -1]);  unsqueeze_7 = None\n",
      "    to_8: \"f32[1, 2, 64, 64]\" = torch.ops.aten.to.dtype_layout(expand_1, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  expand_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:129 in forward, code: four_point_disp = torch.zeros((sz[0], 2, 2, 2)).to(fmap1.device)\n",
      "    zeros: \"f32[1, 2, 2, 2]\" = torch.ops.aten.zeros.default([1, 2, 2, 2], device = device(type='cpu'), pin_memory = False)\n",
      "    to_9: \"f32[1, 2, 2, 2]\" = torch.ops.aten.to.dtype_layout(zeros, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  zeros = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:132 in forward, code: corr = corr_fn(coords1)\n",
      "    permute: \"f32[1, 64, 64, 2]\" = torch.ops.aten.permute.default(to_8, [0, 2, 3, 1])\n",
      "    reshape_1: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.reshape.default(permute, [4096, 1, 1, 2])\n",
      "    div_2: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.div.Tensor(reshape_1, 1);  reshape_1 = None\n",
      "    view_3: \"f32[1, 9, 9, 2]\" = torch.ops.aten.view.default(to_4, [1, 9, 9, 2])\n",
      "    add_8: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.add.Tensor(div_2, view_3);  div_2 = view_3 = None\n",
      "    split_with_sizes = torch.ops.aten.split_with_sizes.default(add_8, [1, 1], -1);  add_8 = None\n",
      "    getitem_6: \"f32[4096, 9, 9, 1]\" = split_with_sizes[0]\n",
      "    getitem_7: \"f32[4096, 9, 9, 1]\" = split_with_sizes[1];  split_with_sizes = None\n",
      "    mul: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_6, 2);  getitem_6 = None\n",
      "    div_3: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul, 63);  mul = None\n",
      "    sub_2: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_3, 1);  div_3 = None\n",
      "    mul_1: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_7, 2);  getitem_7 = None\n",
      "    div_4: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_1, 63);  mul_1 = None\n",
      "    sub_3: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_4, 1);  div_4 = None\n",
      "    cat: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.cat.default([sub_2, sub_3], -1);  sub_2 = sub_3 = None\n",
      "    grid_sampler: \"f32[4096, 1, 9, 9]\" = torch.ops.aten.grid_sampler.default(reshape, cat, 0, 0, True);  reshape = cat = None\n",
      "    view_4: \"f32[1, 64, 64, 81]\" = torch.ops.aten.view.default(grid_sampler, [1, 64, 64, -1]);  grid_sampler = None\n",
      "    reshape_2: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.reshape.default(permute, [4096, 1, 1, 2])\n",
      "    div_5: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.div.Tensor(reshape_2, 2);  reshape_2 = None\n",
      "    view_5: \"f32[1, 9, 9, 2]\" = torch.ops.aten.view.default(to_4, [1, 9, 9, 2])\n",
      "    add_9: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.add.Tensor(div_5, view_5);  div_5 = view_5 = None\n",
      "    split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(add_9, [1, 1], -1);  add_9 = None\n",
      "    getitem_8: \"f32[4096, 9, 9, 1]\" = split_with_sizes_1[0]\n",
      "    getitem_9: \"f32[4096, 9, 9, 1]\" = split_with_sizes_1[1];  split_with_sizes_1 = None\n",
      "    mul_2: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_8, 2);  getitem_8 = None\n",
      "    div_6: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_2, 31);  mul_2 = None\n",
      "    sub_4: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_6, 1);  div_6 = None\n",
      "    mul_3: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_9, 2);  getitem_9 = None\n",
      "    div_7: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_3, 31);  mul_3 = None\n",
      "    sub_5: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_7, 1);  div_7 = None\n",
      "    cat_1: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.cat.default([sub_4, sub_5], -1);  sub_4 = sub_5 = None\n",
      "    grid_sampler_1: \"f32[4096, 1, 9, 9]\" = torch.ops.aten.grid_sampler.default(avg_pool2d, cat_1, 0, 0, True);  avg_pool2d = cat_1 = None\n",
      "    view_6: \"f32[1, 64, 64, 81]\" = torch.ops.aten.view.default(grid_sampler_1, [1, 64, 64, -1]);  grid_sampler_1 = None\n",
      "    reshape_3: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.reshape.default(permute, [4096, 1, 1, 2])\n",
      "    div_8: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.div.Tensor(reshape_3, 4);  reshape_3 = None\n",
      "    view_7: \"f32[1, 9, 9, 2]\" = torch.ops.aten.view.default(to_4, [1, 9, 9, 2])\n",
      "    add_10: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.add.Tensor(div_8, view_7);  div_8 = view_7 = None\n",
      "    split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(add_10, [1, 1], -1);  add_10 = None\n",
      "    getitem_10: \"f32[4096, 9, 9, 1]\" = split_with_sizes_2[0]\n",
      "    getitem_11: \"f32[4096, 9, 9, 1]\" = split_with_sizes_2[1];  split_with_sizes_2 = None\n",
      "    mul_4: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_10, 2);  getitem_10 = None\n",
      "    div_9: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_4, 15);  mul_4 = None\n",
      "    sub_6: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_9, 1);  div_9 = None\n",
      "    mul_5: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_11, 2);  getitem_11 = None\n",
      "    div_10: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_5, 15);  mul_5 = None\n",
      "    sub_7: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_10, 1);  div_10 = None\n",
      "    cat_2: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.cat.default([sub_6, sub_7], -1);  sub_6 = sub_7 = None\n",
      "    grid_sampler_2: \"f32[4096, 1, 9, 9]\" = torch.ops.aten.grid_sampler.default(avg_pool2d_1, cat_2, 0, 0, True);  avg_pool2d_1 = cat_2 = None\n",
      "    view_8: \"f32[1, 64, 64, 81]\" = torch.ops.aten.view.default(grid_sampler_2, [1, 64, 64, -1]);  grid_sampler_2 = None\n",
      "    reshape_4: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.reshape.default(permute, [4096, 1, 1, 2]);  permute = None\n",
      "    div_11: \"f32[4096, 1, 1, 2]\" = torch.ops.aten.div.Tensor(reshape_4, 8);  reshape_4 = None\n",
      "    view_9: \"f32[1, 9, 9, 2]\" = torch.ops.aten.view.default(to_4, [1, 9, 9, 2]);  to_4 = None\n",
      "    add_11: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.add.Tensor(div_11, view_9);  div_11 = view_9 = None\n",
      "    split_with_sizes_3 = torch.ops.aten.split_with_sizes.default(add_11, [1, 1], -1);  add_11 = None\n",
      "    getitem_12: \"f32[4096, 9, 9, 1]\" = split_with_sizes_3[0]\n",
      "    getitem_13: \"f32[4096, 9, 9, 1]\" = split_with_sizes_3[1];  split_with_sizes_3 = None\n",
      "    mul_6: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_12, 2);  getitem_12 = None\n",
      "    div_12: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_6, 7);  mul_6 = None\n",
      "    sub_8: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_12, 1);  div_12 = None\n",
      "    mul_7: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.mul.Tensor(getitem_13, 2);  getitem_13 = None\n",
      "    div_13: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.div.Tensor(mul_7, 7);  mul_7 = None\n",
      "    sub_9: \"f32[4096, 9, 9, 1]\" = torch.ops.aten.sub.Tensor(div_13, 1);  div_13 = None\n",
      "    cat_3: \"f32[4096, 9, 9, 2]\" = torch.ops.aten.cat.default([sub_8, sub_9], -1);  sub_8 = sub_9 = None\n",
      "    grid_sampler_3: \"f32[4096, 1, 9, 9]\" = torch.ops.aten.grid_sampler.default(avg_pool2d_2, cat_3, 0, 0, True);  avg_pool2d_2 = cat_3 = None\n",
      "    view_10: \"f32[1, 64, 64, 81]\" = torch.ops.aten.view.default(grid_sampler_3, [1, 64, 64, -1]);  grid_sampler_3 = None\n",
      "    cat_4: \"f32[1, 64, 64, 324]\" = torch.ops.aten.cat.default([view_4, view_6, view_8, view_10], -1);  view_4 = view_6 = view_8 = view_10 = None\n",
      "    permute_1: \"f32[1, 324, 64, 64]\" = torch.ops.aten.permute.default(cat_4, [0, 3, 1, 2]);  cat_4 = None\n",
      "    contiguous: \"f32[1, 324, 64, 64]\" = torch.ops.aten.contiguous.default(permute_1);  permute_1 = None\n",
      "    to_10: \"f32[1, 324, 64, 64]\" = torch.ops.aten.to.dtype(contiguous, torch.float32);  contiguous = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:133 in forward, code: flow = coords1 - coords0\n",
      "    sub_10: \"f32[1, 2, 64, 64]\" = torch.ops.aten.sub.Tensor(to_8, to_6);  to_8 = to_6 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:134 in forward, code: with _autocast(self.device.type, enabled=self.args.mixed_precision and self.device.type != \"cpu\"):\n",
      "    _enter_autocast_1 = torch.amp.autocast_mode._enter_autocast('cpu', torch.bfloat16, False, False)\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\update.py:327 in forward, code: delta_flow = self.cnn(torch.cat((corr, flow), dim=1))\n",
      "    cat_5: \"f32[1, 326, 64, 64]\" = torch.ops.aten.cat.default([to_10, sub_10], 1);  to_10 = sub_10 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_28: \"f32[1, 128, 64, 64]\" = torch.ops.aten.conv2d.default(cat_5, arg28_1, arg29_1, [1, 1], [1, 1]);  cat_5 = arg28_1 = arg29_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm: \"f32[1, 128, 64, 64]\" = torch.ops.aten.group_norm.default(conv2d_28, 16, arg30_1, arg31_1);  conv2d_28 = arg30_1 = arg31_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_1: \"f32[1, 128, 64, 64]\" = torch.ops.aten.relu.default(group_norm);  group_norm = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
      "    max_pool2d_4: \"f32[1, 128, 32, 32]\" = torch.ops.aten.max_pool2d.default(relu_1, [2, 2], [2, 2]);  relu_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_29: \"f32[1, 128, 32, 32]\" = torch.ops.aten.conv2d.default(max_pool2d_4, arg32_1, arg33_1, [1, 1], [1, 1]);  max_pool2d_4 = arg32_1 = arg33_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm_1: \"f32[1, 128, 32, 32]\" = torch.ops.aten.group_norm.default(conv2d_29, 16, arg34_1, arg35_1);  conv2d_29 = arg34_1 = arg35_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_2: \"f32[1, 128, 32, 32]\" = torch.ops.aten.relu.default(group_norm_1);  group_norm_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
      "    max_pool2d_5: \"f32[1, 128, 16, 16]\" = torch.ops.aten.max_pool2d.default(relu_2, [2, 2], [2, 2]);  relu_2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_30: \"f32[1, 128, 16, 16]\" = torch.ops.aten.conv2d.default(max_pool2d_5, arg36_1, arg37_1, [1, 1], [1, 1]);  max_pool2d_5 = arg36_1 = arg37_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm_2: \"f32[1, 128, 16, 16]\" = torch.ops.aten.group_norm.default(conv2d_30, 16, arg38_1, arg39_1);  conv2d_30 = arg38_1 = arg39_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_3: \"f32[1, 128, 16, 16]\" = torch.ops.aten.relu.default(group_norm_2);  group_norm_2 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
      "    max_pool2d_6: \"f32[1, 128, 8, 8]\" = torch.ops.aten.max_pool2d.default(relu_3, [2, 2], [2, 2]);  relu_3 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_31: \"f32[1, 128, 8, 8]\" = torch.ops.aten.conv2d.default(max_pool2d_6, arg40_1, arg41_1, [1, 1], [1, 1]);  max_pool2d_6 = arg40_1 = arg41_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm_3: \"f32[1, 128, 8, 8]\" = torch.ops.aten.group_norm.default(conv2d_31, 16, arg42_1, arg43_1);  conv2d_31 = arg42_1 = arg43_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_4: \"f32[1, 128, 8, 8]\" = torch.ops.aten.relu.default(group_norm_3);  group_norm_3 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
      "    max_pool2d_7: \"f32[1, 128, 4, 4]\" = torch.ops.aten.max_pool2d.default(relu_4, [2, 2], [2, 2]);  relu_4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_32: \"f32[1, 128, 4, 4]\" = torch.ops.aten.conv2d.default(max_pool2d_7, arg44_1, arg45_1, [1, 1], [1, 1]);  max_pool2d_7 = arg44_1 = arg45_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm_4: \"f32[1, 128, 4, 4]\" = torch.ops.aten.group_norm.default(conv2d_32, 16, arg46_1, arg47_1);  conv2d_32 = arg46_1 = arg47_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_5: \"f32[1, 128, 4, 4]\" = torch.ops.aten.relu.default(group_norm_4);  group_norm_4 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
      "    max_pool2d_8: \"f32[1, 128, 2, 2]\" = torch.ops.aten.max_pool2d.default(relu_5, [2, 2], [2, 2]);  relu_5 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_33: \"f32[1, 128, 2, 2]\" = torch.ops.aten.conv2d.default(max_pool2d_8, arg48_1, arg49_1, [1, 1], [1, 1]);  max_pool2d_8 = arg48_1 = arg49_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:325 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "    group_norm_5: \"f32[1, 128, 2, 2]\" = torch.ops.aten.group_norm.default(conv2d_33, 16, arg50_1, arg51_1);  conv2d_33 = arg50_1 = arg51_1 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
      "    relu_6: \"f32[1, 128, 2, 2]\" = torch.ops.aten.relu.default(group_norm_5);  group_norm_5 = None\n",
      "    \n",
      "     # File: C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d_34: \"f32[1, 2, 2, 2]\" = torch.ops.aten.conv2d.default(relu_6, arg52_1, arg53_1);  relu_6 = arg52_1 = arg53_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:134 in forward, code: with _autocast(self.device.type, enabled=self.args.mixed_precision and self.device.type != \"cpu\"):\n",
      "    _exit_autocast_1 = torch.amp.autocast_mode._exit_autocast(_enter_autocast_1);  _enter_autocast_1 = _exit_autocast_1 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:142 in forward, code: four_point_disp = four_point_disp + delta_four_point\n",
      "    add_12: \"f32[1, 2, 2, 2]\" = torch.ops.aten.add.Tensor(to_9, conv2d_34);  conv2d_34 = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:143 in forward, code: coords1 = self.get_flow_now_4(four_point_disp)\n",
      "    div_14: \"f32[1, 2, 2, 2]\" = torch.ops.aten.div.Tensor(add_12, 4);  add_12 = None\n",
      "    zeros_1: \"f32[2, 2, 2]\" = torch.ops.aten.zeros.default([2, 2, 2], device = device(type='cpu'), pin_memory = False)\n",
      "    to_11: \"f32[2, 2, 2]\" = torch.ops.aten.to.dtype_layout(zeros_1, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  zeros_1 = None\n",
      "    _tensor_constant2: \"f32[2]\" = self._tensor_constant2\n",
      "    lift_fresh_copy_2: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None\n",
      "    select: \"f32[2, 2]\" = torch.ops.aten.select.int(to_11, 1, 0)\n",
      "    select_1: \"f32[2]\" = torch.ops.aten.select.int(select, 1, 0);  select = None\n",
      "    copy_: \"f32[2]\" = torch.ops.aten.copy_.default(select_1, lift_fresh_copy_2);  select_1 = lift_fresh_copy_2 = copy_ = None\n",
      "    _tensor_constant3: \"f32[2]\" = self._tensor_constant3\n",
      "    lift_fresh_copy_3: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None\n",
      "    select_2: \"f32[2, 2]\" = torch.ops.aten.select.int(to_11, 1, 0)\n",
      "    select_3: \"f32[2]\" = torch.ops.aten.select.int(select_2, 1, 1);  select_2 = None\n",
      "    copy__1: \"f32[2]\" = torch.ops.aten.copy_.default(select_3, lift_fresh_copy_3);  select_3 = lift_fresh_copy_3 = copy__1 = None\n",
      "    _tensor_constant4: \"f32[2]\" = self._tensor_constant4\n",
      "    lift_fresh_copy_4: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None\n",
      "    select_4: \"f32[2, 2]\" = torch.ops.aten.select.int(to_11, 1, 1)\n",
      "    select_5: \"f32[2]\" = torch.ops.aten.select.int(select_4, 1, 0);  select_4 = None\n",
      "    copy__2: \"f32[2]\" = torch.ops.aten.copy_.default(select_5, lift_fresh_copy_4);  select_5 = lift_fresh_copy_4 = copy__2 = None\n",
      "    _tensor_constant5: \"f32[2]\" = self._tensor_constant5\n",
      "    lift_fresh_copy_5: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None\n",
      "    select_6: \"f32[2, 2]\" = torch.ops.aten.select.int(to_11, 1, 1)\n",
      "    select_7: \"f32[2]\" = torch.ops.aten.select.int(select_6, 1, 1);  select_6 = None\n",
      "    copy__3: \"f32[2]\" = torch.ops.aten.copy_.default(select_7, lift_fresh_copy_5);  select_7 = lift_fresh_copy_5 = copy__3 = None\n",
      "    unsqueeze_8: \"f32[1, 2, 2, 2]\" = torch.ops.aten.unsqueeze.default(to_11, 0);  to_11 = None\n",
      "    repeat: \"f32[1, 2, 2, 2]\" = torch.ops.aten.repeat.default(unsqueeze_8, [1, 1, 1, 1]);  unsqueeze_8 = None\n",
      "    add_13: \"f32[1, 2, 2, 2]\" = torch.ops.aten.add.Tensor(repeat, div_14);  div_14 = None\n",
      "    flatten: \"f32[1, 2, 4]\" = torch.ops.aten.flatten.using_ints(repeat, 2);  repeat = None\n",
      "    permute_2: \"f32[1, 4, 2]\" = torch.ops.aten.permute.default(flatten, [0, 2, 1]);  flatten = None\n",
      "    contiguous_1: \"f32[1, 4, 2]\" = torch.ops.aten.contiguous.default(permute_2);  permute_2 = None\n",
      "    flatten_1: \"f32[1, 2, 4]\" = torch.ops.aten.flatten.using_ints(add_13, 2);  add_13 = None\n",
      "    permute_3: \"f32[1, 4, 2]\" = torch.ops.aten.permute.default(flatten_1, [0, 2, 1]);  flatten_1 = None\n",
      "    contiguous_2: \"f32[1, 4, 2]\" = torch.ops.aten.contiguous.default(permute_3);  permute_3 = None\n",
      "    empty: \"f32[1, 8, 8]\" = torch.ops.aten.empty.memory_format([1, 8, 8], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    zeros_2: \"f32[1]\" = torch.ops.aten.zeros.default([1], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    ones: \"f32[1]\" = torch.ops.aten.ones.default([1], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    select_8: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 0)\n",
      "    select_9: \"f32[1]\" = torch.ops.aten.select.int(select_8, 1, 0);  select_8 = None\n",
      "    select_10: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 0)\n",
      "    select_11: \"f32[1]\" = torch.ops.aten.select.int(select_10, 1, 1);  select_10 = None\n",
      "    select_12: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 0)\n",
      "    select_13: \"f32[1]\" = torch.ops.aten.select.int(select_12, 1, 0);  select_12 = None\n",
      "    select_14: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 0)\n",
      "    select_15: \"f32[1]\" = torch.ops.aten.select.int(select_14, 1, 1);  select_14 = None\n",
      "    neg: \"f32[1]\" = torch.ops.aten.neg.default(select_9)\n",
      "    mul_8: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg, select_13);  neg = None\n",
      "    neg_1: \"f32[1]\" = torch.ops.aten.neg.default(select_11)\n",
      "    mul_9: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_1, select_13);  neg_1 = select_13 = None\n",
      "    stack_3: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_9, select_11, ones, zeros_2, zeros_2, zeros_2, mul_8, mul_9], -1);  mul_8 = mul_9 = None\n",
      "    select_16: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 0)\n",
      "    copy__4: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_16, stack_3);  select_16 = stack_3 = copy__4 = None\n",
      "    neg_2: \"f32[1]\" = torch.ops.aten.neg.default(select_9)\n",
      "    mul_10: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_2, select_15);  neg_2 = None\n",
      "    neg_3: \"f32[1]\" = torch.ops.aten.neg.default(select_11)\n",
      "    mul_11: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_3, select_15);  neg_3 = select_15 = None\n",
      "    stack_4: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_2, zeros_2, zeros_2, select_9, select_11, ones, mul_10, mul_11], -1);  select_9 = select_11 = mul_10 = mul_11 = None\n",
      "    select_17: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 1)\n",
      "    copy__5: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_17, stack_4);  select_17 = stack_4 = copy__5 = None\n",
      "    select_18: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 1)\n",
      "    select_19: \"f32[1]\" = torch.ops.aten.select.int(select_18, 1, 0);  select_18 = None\n",
      "    select_20: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 1)\n",
      "    select_21: \"f32[1]\" = torch.ops.aten.select.int(select_20, 1, 1);  select_20 = None\n",
      "    select_22: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 1)\n",
      "    select_23: \"f32[1]\" = torch.ops.aten.select.int(select_22, 1, 0);  select_22 = None\n",
      "    select_24: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 1)\n",
      "    select_25: \"f32[1]\" = torch.ops.aten.select.int(select_24, 1, 1);  select_24 = None\n",
      "    neg_4: \"f32[1]\" = torch.ops.aten.neg.default(select_19)\n",
      "    mul_12: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_4, select_23);  neg_4 = None\n",
      "    neg_5: \"f32[1]\" = torch.ops.aten.neg.default(select_21)\n",
      "    mul_13: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_5, select_23);  neg_5 = select_23 = None\n",
      "    stack_5: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_19, select_21, ones, zeros_2, zeros_2, zeros_2, mul_12, mul_13], -1);  mul_12 = mul_13 = None\n",
      "    select_26: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 2)\n",
      "    copy__6: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_26, stack_5);  select_26 = stack_5 = copy__6 = None\n",
      "    neg_6: \"f32[1]\" = torch.ops.aten.neg.default(select_19)\n",
      "    mul_14: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_6, select_25);  neg_6 = None\n",
      "    neg_7: \"f32[1]\" = torch.ops.aten.neg.default(select_21)\n",
      "    mul_15: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_7, select_25);  neg_7 = select_25 = None\n",
      "    stack_6: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_2, zeros_2, zeros_2, select_19, select_21, ones, mul_14, mul_15], -1);  select_19 = select_21 = mul_14 = mul_15 = None\n",
      "    select_27: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 3)\n",
      "    copy__7: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_27, stack_6);  select_27 = stack_6 = copy__7 = None\n",
      "    select_28: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 2)\n",
      "    select_29: \"f32[1]\" = torch.ops.aten.select.int(select_28, 1, 0);  select_28 = None\n",
      "    select_30: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 2)\n",
      "    select_31: \"f32[1]\" = torch.ops.aten.select.int(select_30, 1, 1);  select_30 = None\n",
      "    select_32: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 2)\n",
      "    select_33: \"f32[1]\" = torch.ops.aten.select.int(select_32, 1, 0);  select_32 = None\n",
      "    select_34: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 2)\n",
      "    select_35: \"f32[1]\" = torch.ops.aten.select.int(select_34, 1, 1);  select_34 = None\n",
      "    neg_8: \"f32[1]\" = torch.ops.aten.neg.default(select_29)\n",
      "    mul_16: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_8, select_33);  neg_8 = None\n",
      "    neg_9: \"f32[1]\" = torch.ops.aten.neg.default(select_31)\n",
      "    mul_17: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_9, select_33);  neg_9 = select_33 = None\n",
      "    stack_7: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_29, select_31, ones, zeros_2, zeros_2, zeros_2, mul_16, mul_17], -1);  mul_16 = mul_17 = None\n",
      "    select_36: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 4)\n",
      "    copy__8: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_36, stack_7);  select_36 = stack_7 = copy__8 = None\n",
      "    neg_10: \"f32[1]\" = torch.ops.aten.neg.default(select_29)\n",
      "    mul_18: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_10, select_35);  neg_10 = None\n",
      "    neg_11: \"f32[1]\" = torch.ops.aten.neg.default(select_31)\n",
      "    mul_19: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_11, select_35);  neg_11 = select_35 = None\n",
      "    stack_8: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_2, zeros_2, zeros_2, select_29, select_31, ones, mul_18, mul_19], -1);  select_29 = select_31 = mul_18 = mul_19 = None\n",
      "    select_37: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 5)\n",
      "    copy__9: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_37, stack_8);  select_37 = stack_8 = copy__9 = None\n",
      "    select_38: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 3)\n",
      "    select_39: \"f32[1]\" = torch.ops.aten.select.int(select_38, 1, 0);  select_38 = None\n",
      "    select_40: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_1, 1, 3);  contiguous_1 = None\n",
      "    select_41: \"f32[1]\" = torch.ops.aten.select.int(select_40, 1, 1);  select_40 = None\n",
      "    select_42: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 3)\n",
      "    select_43: \"f32[1]\" = torch.ops.aten.select.int(select_42, 1, 0);  select_42 = None\n",
      "    select_44: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_2, 1, 3)\n",
      "    select_45: \"f32[1]\" = torch.ops.aten.select.int(select_44, 1, 1);  select_44 = None\n",
      "    neg_12: \"f32[1]\" = torch.ops.aten.neg.default(select_39)\n",
      "    mul_20: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_12, select_43);  neg_12 = None\n",
      "    neg_13: \"f32[1]\" = torch.ops.aten.neg.default(select_41)\n",
      "    mul_21: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_13, select_43);  neg_13 = select_43 = None\n",
      "    stack_9: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_39, select_41, ones, zeros_2, zeros_2, zeros_2, mul_20, mul_21], -1);  mul_20 = mul_21 = None\n",
      "    select_46: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 6)\n",
      "    copy__10: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_46, stack_9);  select_46 = stack_9 = copy__10 = None\n",
      "    neg_14: \"f32[1]\" = torch.ops.aten.neg.default(select_39)\n",
      "    mul_22: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_14, select_45);  neg_14 = None\n",
      "    neg_15: \"f32[1]\" = torch.ops.aten.neg.default(select_41)\n",
      "    mul_23: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_15, select_45);  neg_15 = select_45 = None\n",
      "    stack_10: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_2, zeros_2, zeros_2, select_39, select_41, ones, mul_22, mul_23], -1);  zeros_2 = select_39 = select_41 = ones = mul_22 = mul_23 = None\n",
      "    select_47: \"f32[1, 8]\" = torch.ops.aten.select.int(empty, 1, 7)\n",
      "    copy__11: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_47, stack_10);  select_47 = stack_10 = copy__11 = None\n",
      "    view_11: \"f32[1, 8, 1]\" = torch.ops.aten.view.default(contiguous_2, [-1, 8, 1]);  contiguous_2 = None\n",
      "    to_12: \"f64[1, 8, 8]\" = torch.ops.aten.to.dtype(empty, torch.float64);  empty = None\n",
      "    to_13: \"f64[1, 8, 1]\" = torch.ops.aten.to.dtype(view_11, torch.float64);  view_11 = None\n",
      "    linalg_solve: \"f64[1, 8, 1]\" = torch.ops.aten.linalg_solve.default(to_12, to_13);  to_12 = to_13 = None\n",
      "    to_14: \"f32[1, 8, 1]\" = torch.ops.aten.to.dtype(linalg_solve, torch.float32);  linalg_solve = None\n",
      "    empty_1: \"f32[1, 9]\" = torch.ops.aten.empty.memory_format([1, 9], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    select_48: \"f32[1, 8]\" = torch.ops.aten.select.int(to_14, 2, 0);  to_14 = None\n",
      "    slice_1: \"f32[1, 8]\" = torch.ops.aten.slice.Tensor(empty_1, 1, 0, 8)\n",
      "    copy__12: \"f32[1, 8]\" = torch.ops.aten.copy_.default(slice_1, select_48);  slice_1 = select_48 = copy__12 = None\n",
      "    select_49: \"f32[1]\" = torch.ops.aten.select.int(empty_1, 1, -1)\n",
      "    fill_: \"f32[1]\" = torch.ops.aten.fill_.Scalar(select_49, 1);  select_49 = fill_ = None\n",
      "    view_12: \"f32[1, 3, 3]\" = torch.ops.aten.view.default(empty_1, [-1, 3, 3]);  empty_1 = None\n",
      "    linspace_2: \"f32[64]\" = torch.ops.aten.linspace.default(0, 63, 64, device = device(type='cpu'), pin_memory = False)\n",
      "    linspace_3: \"f32[64]\" = torch.ops.aten.linspace.default(0, 63, 64, device = device(type='cpu'), pin_memory = False)\n",
      "    meshgrid_3 = torch.ops.aten.meshgrid.default([linspace_2, linspace_3]);  linspace_2 = linspace_3 = None\n",
      "    getitem_14: \"f32[64, 64]\" = meshgrid_3[0]\n",
      "    getitem_15: \"f32[64, 64]\" = meshgrid_3[1];  meshgrid_3 = None\n",
      "    flatten_2: \"f32[4096]\" = torch.ops.aten.flatten.using_ints(getitem_15);  getitem_15 = None\n",
      "    unsqueeze_9: \"f32[1, 4096]\" = torch.ops.aten.unsqueeze.default(flatten_2, 0);  flatten_2 = None\n",
      "    flatten_3: \"f32[4096]\" = torch.ops.aten.flatten.using_ints(getitem_14);  getitem_14 = None\n",
      "    unsqueeze_10: \"f32[1, 4096]\" = torch.ops.aten.unsqueeze.default(flatten_3, 0);  flatten_3 = None\n",
      "    ones_1: \"f32[1, 4096]\" = torch.ops.aten.ones.default([1, 4096], device = device(type='cpu'), pin_memory = False)\n",
      "    cat_6: \"f32[3, 4096]\" = torch.ops.aten.cat.default([unsqueeze_9, unsqueeze_10, ones_1]);  unsqueeze_9 = unsqueeze_10 = ones_1 = None\n",
      "    unsqueeze_11: \"f32[1, 3, 4096]\" = torch.ops.aten.unsqueeze.default(cat_6, 0);  cat_6 = None\n",
      "    repeat_1: \"f32[1, 3, 4096]\" = torch.ops.aten.repeat.default(unsqueeze_11, [1, 1, 1]);  unsqueeze_11 = None\n",
      "    to_15: \"f32[1, 3, 4096]\" = torch.ops.aten.to.dtype_layout(repeat_1, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  repeat_1 = None\n",
      "    bmm: \"f32[1, 3, 4096]\" = torch.ops.aten.bmm.default(view_12, to_15);  view_12 = to_15 = None\n",
      "    isnan: \"b8[1, 3, 4096]\" = torch.ops.aten.isnan.default(bmm);  bmm = None\n",
      "    any_1: \"b8[]\" = torch.ops.aten.any.default(isnan);  isnan = None\n",
      "    ne: \"b8[]\" = torch.ops.aten.ne.Scalar(any_1, 0);  any_1 = None\n",
      "    item: \"Sym(Eq(u0, 1))\" = torch.ops.aten.item.default(ne);  ne = item = None\n",
      "    \n",
      "     # File: d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py:149 in forward, code: coords1 = self.get_flow_now_4(four_point_disp)\n",
      "    div_15: \"f32[1, 2, 2, 2]\" = torch.ops.aten.div.Tensor(to_9, 4);  to_9 = None\n",
      "    zeros_3: \"f32[2, 2, 2]\" = torch.ops.aten.zeros.default([2, 2, 2], device = device(type='cpu'), pin_memory = False)\n",
      "    to_16: \"f32[2, 2, 2]\" = torch.ops.aten.to.dtype_layout(zeros_3, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  zeros_3 = None\n",
      "    _tensor_constant6: \"f32[2]\" = self._tensor_constant6\n",
      "    lift_fresh_copy_6: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None\n",
      "    select_50: \"f32[2, 2]\" = torch.ops.aten.select.int(to_16, 1, 0)\n",
      "    select_51: \"f32[2]\" = torch.ops.aten.select.int(select_50, 1, 0);  select_50 = None\n",
      "    copy__13: \"f32[2]\" = torch.ops.aten.copy_.default(select_51, lift_fresh_copy_6);  select_51 = lift_fresh_copy_6 = copy__13 = None\n",
      "    _tensor_constant7: \"f32[2]\" = self._tensor_constant7\n",
      "    lift_fresh_copy_7: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant7);  _tensor_constant7 = None\n",
      "    select_52: \"f32[2, 2]\" = torch.ops.aten.select.int(to_16, 1, 0)\n",
      "    select_53: \"f32[2]\" = torch.ops.aten.select.int(select_52, 1, 1);  select_52 = None\n",
      "    copy__14: \"f32[2]\" = torch.ops.aten.copy_.default(select_53, lift_fresh_copy_7);  select_53 = lift_fresh_copy_7 = copy__14 = None\n",
      "    _tensor_constant8: \"f32[2]\" = self._tensor_constant8\n",
      "    lift_fresh_copy_8: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant8);  _tensor_constant8 = None\n",
      "    select_54: \"f32[2, 2]\" = torch.ops.aten.select.int(to_16, 1, 1)\n",
      "    select_55: \"f32[2]\" = torch.ops.aten.select.int(select_54, 1, 0);  select_54 = None\n",
      "    copy__15: \"f32[2]\" = torch.ops.aten.copy_.default(select_55, lift_fresh_copy_8);  select_55 = lift_fresh_copy_8 = copy__15 = None\n",
      "    _tensor_constant9: \"f32[2]\" = self._tensor_constant9\n",
      "    lift_fresh_copy_9: \"f32[2]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant9);  _tensor_constant9 = None\n",
      "    select_56: \"f32[2, 2]\" = torch.ops.aten.select.int(to_16, 1, 1)\n",
      "    select_57: \"f32[2]\" = torch.ops.aten.select.int(select_56, 1, 1);  select_56 = None\n",
      "    copy__16: \"f32[2]\" = torch.ops.aten.copy_.default(select_57, lift_fresh_copy_9);  select_57 = lift_fresh_copy_9 = copy__16 = None\n",
      "    unsqueeze_12: \"f32[1, 2, 2, 2]\" = torch.ops.aten.unsqueeze.default(to_16, 0);  to_16 = None\n",
      "    repeat_2: \"f32[1, 2, 2, 2]\" = torch.ops.aten.repeat.default(unsqueeze_12, [1, 1, 1, 1]);  unsqueeze_12 = None\n",
      "    add_14: \"f32[1, 2, 2, 2]\" = torch.ops.aten.add.Tensor(repeat_2, div_15);  div_15 = None\n",
      "    flatten_4: \"f32[1, 2, 4]\" = torch.ops.aten.flatten.using_ints(repeat_2, 2);  repeat_2 = None\n",
      "    permute_4: \"f32[1, 4, 2]\" = torch.ops.aten.permute.default(flatten_4, [0, 2, 1]);  flatten_4 = None\n",
      "    contiguous_3: \"f32[1, 4, 2]\" = torch.ops.aten.contiguous.default(permute_4);  permute_4 = None\n",
      "    flatten_5: \"f32[1, 2, 4]\" = torch.ops.aten.flatten.using_ints(add_14, 2);  add_14 = None\n",
      "    permute_5: \"f32[1, 4, 2]\" = torch.ops.aten.permute.default(flatten_5, [0, 2, 1]);  flatten_5 = None\n",
      "    contiguous_4: \"f32[1, 4, 2]\" = torch.ops.aten.contiguous.default(permute_5);  permute_5 = None\n",
      "    empty_2: \"f32[1, 8, 8]\" = torch.ops.aten.empty.memory_format([1, 8, 8], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    zeros_4: \"f32[1]\" = torch.ops.aten.zeros.default([1], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    ones_2: \"f32[1]\" = torch.ops.aten.ones.default([1], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    select_58: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 0)\n",
      "    select_59: \"f32[1]\" = torch.ops.aten.select.int(select_58, 1, 0);  select_58 = None\n",
      "    select_60: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 0)\n",
      "    select_61: \"f32[1]\" = torch.ops.aten.select.int(select_60, 1, 1);  select_60 = None\n",
      "    select_62: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 0)\n",
      "    select_63: \"f32[1]\" = torch.ops.aten.select.int(select_62, 1, 0);  select_62 = None\n",
      "    select_64: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 0)\n",
      "    select_65: \"f32[1]\" = torch.ops.aten.select.int(select_64, 1, 1);  select_64 = None\n",
      "    neg_16: \"f32[1]\" = torch.ops.aten.neg.default(select_59)\n",
      "    mul_24: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_16, select_63);  neg_16 = None\n",
      "    neg_17: \"f32[1]\" = torch.ops.aten.neg.default(select_61)\n",
      "    mul_25: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_17, select_63);  neg_17 = select_63 = None\n",
      "    stack_11: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_59, select_61, ones_2, zeros_4, zeros_4, zeros_4, mul_24, mul_25], -1);  mul_24 = mul_25 = None\n",
      "    select_66: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 0)\n",
      "    copy__17: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_66, stack_11);  select_66 = stack_11 = copy__17 = None\n",
      "    neg_18: \"f32[1]\" = torch.ops.aten.neg.default(select_59)\n",
      "    mul_26: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_18, select_65);  neg_18 = None\n",
      "    neg_19: \"f32[1]\" = torch.ops.aten.neg.default(select_61)\n",
      "    mul_27: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_19, select_65);  neg_19 = select_65 = None\n",
      "    stack_12: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_4, zeros_4, zeros_4, select_59, select_61, ones_2, mul_26, mul_27], -1);  select_59 = select_61 = mul_26 = mul_27 = None\n",
      "    select_67: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 1)\n",
      "    copy__18: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_67, stack_12);  select_67 = stack_12 = copy__18 = None\n",
      "    select_68: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 1)\n",
      "    select_69: \"f32[1]\" = torch.ops.aten.select.int(select_68, 1, 0);  select_68 = None\n",
      "    select_70: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 1)\n",
      "    select_71: \"f32[1]\" = torch.ops.aten.select.int(select_70, 1, 1);  select_70 = None\n",
      "    select_72: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 1)\n",
      "    select_73: \"f32[1]\" = torch.ops.aten.select.int(select_72, 1, 0);  select_72 = None\n",
      "    select_74: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 1)\n",
      "    select_75: \"f32[1]\" = torch.ops.aten.select.int(select_74, 1, 1);  select_74 = None\n",
      "    neg_20: \"f32[1]\" = torch.ops.aten.neg.default(select_69)\n",
      "    mul_28: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_20, select_73);  neg_20 = None\n",
      "    neg_21: \"f32[1]\" = torch.ops.aten.neg.default(select_71)\n",
      "    mul_29: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_21, select_73);  neg_21 = select_73 = None\n",
      "    stack_13: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_69, select_71, ones_2, zeros_4, zeros_4, zeros_4, mul_28, mul_29], -1);  mul_28 = mul_29 = None\n",
      "    select_76: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 2)\n",
      "    copy__19: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_76, stack_13);  select_76 = stack_13 = copy__19 = None\n",
      "    neg_22: \"f32[1]\" = torch.ops.aten.neg.default(select_69)\n",
      "    mul_30: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_22, select_75);  neg_22 = None\n",
      "    neg_23: \"f32[1]\" = torch.ops.aten.neg.default(select_71)\n",
      "    mul_31: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_23, select_75);  neg_23 = select_75 = None\n",
      "    stack_14: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_4, zeros_4, zeros_4, select_69, select_71, ones_2, mul_30, mul_31], -1);  select_69 = select_71 = mul_30 = mul_31 = None\n",
      "    select_77: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 3)\n",
      "    copy__20: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_77, stack_14);  select_77 = stack_14 = copy__20 = None\n",
      "    select_78: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 2)\n",
      "    select_79: \"f32[1]\" = torch.ops.aten.select.int(select_78, 1, 0);  select_78 = None\n",
      "    select_80: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 2)\n",
      "    select_81: \"f32[1]\" = torch.ops.aten.select.int(select_80, 1, 1);  select_80 = None\n",
      "    select_82: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 2)\n",
      "    select_83: \"f32[1]\" = torch.ops.aten.select.int(select_82, 1, 0);  select_82 = None\n",
      "    select_84: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 2)\n",
      "    select_85: \"f32[1]\" = torch.ops.aten.select.int(select_84, 1, 1);  select_84 = None\n",
      "    neg_24: \"f32[1]\" = torch.ops.aten.neg.default(select_79)\n",
      "    mul_32: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_24, select_83);  neg_24 = None\n",
      "    neg_25: \"f32[1]\" = torch.ops.aten.neg.default(select_81)\n",
      "    mul_33: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_25, select_83);  neg_25 = select_83 = None\n",
      "    stack_15: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_79, select_81, ones_2, zeros_4, zeros_4, zeros_4, mul_32, mul_33], -1);  mul_32 = mul_33 = None\n",
      "    select_86: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 4)\n",
      "    copy__21: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_86, stack_15);  select_86 = stack_15 = copy__21 = None\n",
      "    neg_26: \"f32[1]\" = torch.ops.aten.neg.default(select_79)\n",
      "    mul_34: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_26, select_85);  neg_26 = None\n",
      "    neg_27: \"f32[1]\" = torch.ops.aten.neg.default(select_81)\n",
      "    mul_35: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_27, select_85);  neg_27 = select_85 = None\n",
      "    stack_16: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_4, zeros_4, zeros_4, select_79, select_81, ones_2, mul_34, mul_35], -1);  select_79 = select_81 = mul_34 = mul_35 = None\n",
      "    select_87: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 5)\n",
      "    copy__22: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_87, stack_16);  select_87 = stack_16 = copy__22 = None\n",
      "    select_88: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 3)\n",
      "    select_89: \"f32[1]\" = torch.ops.aten.select.int(select_88, 1, 0);  select_88 = None\n",
      "    select_90: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_3, 1, 3);  contiguous_3 = None\n",
      "    select_91: \"f32[1]\" = torch.ops.aten.select.int(select_90, 1, 1);  select_90 = None\n",
      "    select_92: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 3)\n",
      "    select_93: \"f32[1]\" = torch.ops.aten.select.int(select_92, 1, 0);  select_92 = None\n",
      "    select_94: \"f32[1, 2]\" = torch.ops.aten.select.int(contiguous_4, 1, 3)\n",
      "    select_95: \"f32[1]\" = torch.ops.aten.select.int(select_94, 1, 1);  select_94 = None\n",
      "    neg_28: \"f32[1]\" = torch.ops.aten.neg.default(select_89)\n",
      "    mul_36: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_28, select_93);  neg_28 = None\n",
      "    neg_29: \"f32[1]\" = torch.ops.aten.neg.default(select_91)\n",
      "    mul_37: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_29, select_93);  neg_29 = select_93 = None\n",
      "    stack_17: \"f32[1, 8]\" = torch.ops.aten.stack.default([select_89, select_91, ones_2, zeros_4, zeros_4, zeros_4, mul_36, mul_37], -1);  mul_36 = mul_37 = None\n",
      "    select_96: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 6)\n",
      "    copy__23: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_96, stack_17);  select_96 = stack_17 = copy__23 = None\n",
      "    neg_30: \"f32[1]\" = torch.ops.aten.neg.default(select_89)\n",
      "    mul_38: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_30, select_95);  neg_30 = None\n",
      "    neg_31: \"f32[1]\" = torch.ops.aten.neg.default(select_91)\n",
      "    mul_39: \"f32[1]\" = torch.ops.aten.mul.Tensor(neg_31, select_95);  neg_31 = select_95 = None\n",
      "    stack_18: \"f32[1, 8]\" = torch.ops.aten.stack.default([zeros_4, zeros_4, zeros_4, select_89, select_91, ones_2, mul_38, mul_39], -1);  zeros_4 = select_89 = select_91 = ones_2 = mul_38 = mul_39 = None\n",
      "    select_97: \"f32[1, 8]\" = torch.ops.aten.select.int(empty_2, 1, 7)\n",
      "    copy__24: \"f32[1, 8]\" = torch.ops.aten.copy_.default(select_97, stack_18);  select_97 = stack_18 = copy__24 = None\n",
      "    view_13: \"f32[1, 8, 1]\" = torch.ops.aten.view.default(contiguous_4, [-1, 8, 1]);  contiguous_4 = None\n",
      "    to_17: \"f64[1, 8, 8]\" = torch.ops.aten.to.dtype(empty_2, torch.float64);  empty_2 = None\n",
      "    to_18: \"f64[1, 8, 1]\" = torch.ops.aten.to.dtype(view_13, torch.float64);  view_13 = None\n",
      "    linalg_solve_1: \"f64[1, 8, 1]\" = torch.ops.aten.linalg_solve.default(to_17, to_18);  to_17 = to_18 = None\n",
      "    to_19: \"f32[1, 8, 1]\" = torch.ops.aten.to.dtype(linalg_solve_1, torch.float32);  linalg_solve_1 = None\n",
      "    empty_3: \"f32[1, 9]\" = torch.ops.aten.empty.memory_format([1, 9], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    select_98: \"f32[1, 8]\" = torch.ops.aten.select.int(to_19, 2, 0);  to_19 = None\n",
      "    slice_2: \"f32[1, 8]\" = torch.ops.aten.slice.Tensor(empty_3, 1, 0, 8)\n",
      "    copy__25: \"f32[1, 8]\" = torch.ops.aten.copy_.default(slice_2, select_98);  slice_2 = select_98 = copy__25 = None\n",
      "    select_99: \"f32[1]\" = torch.ops.aten.select.int(empty_3, 1, -1)\n",
      "    fill__1: \"f32[1]\" = torch.ops.aten.fill_.Scalar(select_99, 1);  select_99 = fill__1 = None\n",
      "    view_14: \"f32[1, 3, 3]\" = torch.ops.aten.view.default(empty_3, [-1, 3, 3]);  empty_3 = None\n",
      "    linspace_4: \"f32[64]\" = torch.ops.aten.linspace.default(0, 63, 64, device = device(type='cpu'), pin_memory = False)\n",
      "    linspace_5: \"f32[64]\" = torch.ops.aten.linspace.default(0, 63, 64, device = device(type='cpu'), pin_memory = False)\n",
      "    meshgrid_4 = torch.ops.aten.meshgrid.default([linspace_4, linspace_5]);  linspace_4 = linspace_5 = None\n",
      "    getitem_16: \"f32[64, 64]\" = meshgrid_4[0]\n",
      "    getitem_17: \"f32[64, 64]\" = meshgrid_4[1];  meshgrid_4 = None\n",
      "    flatten_6: \"f32[4096]\" = torch.ops.aten.flatten.using_ints(getitem_17);  getitem_17 = None\n",
      "    unsqueeze_13: \"f32[1, 4096]\" = torch.ops.aten.unsqueeze.default(flatten_6, 0);  flatten_6 = None\n",
      "    flatten_7: \"f32[4096]\" = torch.ops.aten.flatten.using_ints(getitem_16);  getitem_16 = None\n",
      "    unsqueeze_14: \"f32[1, 4096]\" = torch.ops.aten.unsqueeze.default(flatten_7, 0);  flatten_7 = None\n",
      "    ones_3: \"f32[1, 4096]\" = torch.ops.aten.ones.default([1, 4096], device = device(type='cpu'), pin_memory = False)\n",
      "    cat_7: \"f32[3, 4096]\" = torch.ops.aten.cat.default([unsqueeze_13, unsqueeze_14, ones_3]);  unsqueeze_13 = unsqueeze_14 = ones_3 = None\n",
      "    unsqueeze_15: \"f32[1, 3, 4096]\" = torch.ops.aten.unsqueeze.default(cat_7, 0);  cat_7 = None\n",
      "    repeat_3: \"f32[1, 3, 4096]\" = torch.ops.aten.repeat.default(unsqueeze_15, [1, 1, 1]);  unsqueeze_15 = None\n",
      "    to_20: \"f32[1, 3, 4096]\" = torch.ops.aten.to.dtype_layout(repeat_3, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  repeat_3 = None\n",
      "    bmm_1: \"f32[1, 3, 4096]\" = torch.ops.aten.bmm.default(view_14, to_20);  view_14 = to_20 = None\n",
      "    isnan_1: \"b8[1, 3, 4096]\" = torch.ops.aten.isnan.default(bmm_1);  bmm_1 = None\n",
      "    any_2: \"b8[]\" = torch.ops.aten.any.default(isnan_1);  isnan_1 = None\n",
      "    ne_1: \"b8[]\" = torch.ops.aten.ne.Scalar(any_2, 0);  any_2 = None\n",
      "    item_1: \"Sym(Eq(u1, 1))\" = torch.ops.aten.item.default(ne_1);  ne_1 = item_1 = None\n",
      "    \n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    coords1 = self.get_flow_now_4(four_point_disp)\n",
      "  File \u001b[35m\"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py\"\u001b[0m, line \u001b[35m60\u001b[0m, in \u001b[35mget_flow_now_4\u001b[0m\n",
      "    if \u001b[31mtorch.isnan(points_new).any\u001b[0m\u001b[1;31m()\u001b[0m:\n",
      "       \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m1409\u001b[0m, in \u001b[35m__torch_function__\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m1479\u001b[0m, in \u001b[35m__torch_function__\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\_export\\non_strict_utils.py\"\u001b[0m, line \u001b[35m1066\u001b[0m, in \u001b[35m__torch_function__\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\sym_node.py\"\u001b[0m, line \u001b[35m538\u001b[0m, in \u001b[35mguard_bool\u001b[0m\n",
      "    r = self.evaluate()\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\sym_node.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mevaluate\u001b[0m\n",
      "    return \u001b[31mself.shape_env.evaluate_sym_node\u001b[0m\u001b[1;31m(self, size_oblivious)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py\"\u001b[0m, line \u001b[35m7233\u001b[0m, in \u001b[35mevaluate_sym_node\u001b[0m\n",
      "    return \u001b[31mself.evaluate_expr\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31msym_node.expr,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<3 lines>...\n",
      "        \u001b[1;31mfallback_value=fallback_value,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py\"\u001b[0m, line \u001b[35m7333\u001b[0m, in \u001b[35mevaluate_expr\u001b[0m\n",
      "    return \u001b[31mself._inner_evaluate_expr\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31morig_expr,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    ...<5 lines>...\n",
      "        \u001b[1;31mfallback_value,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\recording.py\"\u001b[0m, line \u001b[35m272\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return retlog(\u001b[31mfn\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m)\n",
      "                  \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py\"\u001b[0m, line \u001b[35m7356\u001b[0m, in \u001b[35m_inner_evaluate_expr\u001b[0m\n",
      "    return \u001b[31mself._evaluate_expr\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31morig_expr,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    ...<4 lines>...\n",
      "        \u001b[1;31mforcing_spec=forcing_spec,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py\"\u001b[0m, line \u001b[35m7574\u001b[0m, in \u001b[35m_evaluate_expr\u001b[0m\n",
      "    raise self._make_data_dependent_error(\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "\u001b[1;35mtorch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode\u001b[0m: \u001b[35mCould not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none)\n",
      "\n",
      "consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export\\non_strict_utils.py:1066 in __torch_function__)\n",
      "For more information, run with TORCH_LOGS=\"dynamic\"\n",
      "For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\"\n",
      "If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
      "For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n",
      "\n",
      "For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
      "\n",
      "The following call raised this error:\n",
      "  File \"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py\", line 60, in get_flow_now_4\n",
      "    if torch.isnan(points_new).any():\n",
      "\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_capture_strategies.py\"\u001b[0m, line \u001b[35m118\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    exported_program = self._capture(model, args, kwargs, dynamic_shapes)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_capture_strategies.py\"\u001b[0m, line \u001b[35m210\u001b[0m, in \u001b[35m_capture\u001b[0m\n",
      "    return \u001b[31mtorch.export.export\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^\u001b[0m\n",
      "    ...<3 lines>...\n",
      "        \u001b[1;31mstrict=False,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\__init__.py\"\u001b[0m, line \u001b[35m311\u001b[0m, in \u001b[35mexport\u001b[0m\n",
      "    raise e\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\__init__.py\"\u001b[0m, line \u001b[35m277\u001b[0m, in \u001b[35mexport\u001b[0m\n",
      "    return _export(\n",
      "        mod,\n",
      "    ...<6 lines>...\n",
      "        prefer_deferred_runtime_asserts_over_guards=prefer_deferred_runtime_asserts_over_guards,\n",
      "    )\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m1163\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    raise e\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m1129\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    ep = fn(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\exported_program.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return fn(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m2255\u001b[0m, in \u001b[35m_export\u001b[0m\n",
      "    ep = _export_for_training(\n",
      "        mod,\n",
      "    ...<5 lines>...\n",
      "        prefer_deferred_runtime_asserts_over_guards=prefer_deferred_runtime_asserts_over_guards,\n",
      "    )\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m1163\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    raise e\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m1129\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    ep = fn(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\exported_program.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return fn(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m2071\u001b[0m, in \u001b[35m_export_for_training\u001b[0m\n",
      "    export_artifact = export_func(\n",
      "        mod=mod,\n",
      "    ...<6 lines>...\n",
      "        _to_aten_func=_export_to_aten_ir_make_fx,\n",
      "    )\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m2002\u001b[0m, in \u001b[35m_non_strict_export\u001b[0m\n",
      "    aten_export_artifact = _to_aten_func(  # type: ignore[operator]\n",
      "        patched_mod,\n",
      "    ...<5 lines>...\n",
      "        transform=_tuplify_outputs,\n",
      "    )\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m1793\u001b[0m, in \u001b[35m_export_to_aten_ir_make_fx\u001b[0m\n",
      "    gm, graph_signature = \u001b[31mtransform(_make_fx_helper)\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                          \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmod,\u001b[0m\n",
      "        \u001b[1;31m^^^^\u001b[0m\n",
      "    ...<2 lines>...\n",
      "        \u001b[1;31mkwargs=fake_kwargs,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m1922\u001b[0m, in \u001b[35m_aot_export_non_strict\u001b[0m\n",
      "    gm, sig = \u001b[31maot_export\u001b[0m\u001b[1;31m(wrapped_mod, args, kwargs=kwargs, **flags)\u001b[0m\n",
      "              \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m1706\u001b[0m, in \u001b[35m_make_fx_helper\u001b[0m\n",
      "    gm = make_fx(\n",
      "    ...<2 lines>...\n",
      "        pre_dispatch=True,\n",
      "    )(*flat_args)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m2429\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mmake_fx_tracer.trace\u001b[0m\u001b[1;31m(f, *args)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m2356\u001b[0m, in \u001b[35mtrace\u001b[0m\n",
      "    return \u001b[31mself._trace_inner\u001b[0m\u001b[1;31m(f, *args)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m2318\u001b[0m, in \u001b[35m_trace_inner\u001b[0m\n",
      "    t = dispatch_trace(\n",
      "        wrap_key(func, args, self.fx_tracer, self.pre_dispatch),\n",
      "        tracer=self.fx_tracer,\n",
      "        concrete_args=tuple(phs),\n",
      "    )\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\_compile.py\"\u001b[0m, line \u001b[35m53\u001b[0m, in \u001b[35minner\u001b[0m\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\"\u001b[0m, line \u001b[35m1044\u001b[0m, in \u001b[35m_fn\u001b[0m\n",
      "    return fn(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m1303\u001b[0m, in \u001b[35mdispatch_trace\u001b[0m\n",
      "    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m1908\u001b[0m, in \u001b[35mtrace\u001b[0m\n",
      "    res = super().trace(root, concrete_args)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py\"\u001b[0m, line \u001b[35m868\u001b[0m, in \u001b[35mtrace\u001b[0m\n",
      "    (self.create_arg(\u001b[31mfn\u001b[0m\u001b[1;31m(*args)\u001b[0m),),\n",
      "                     \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m1361\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    out = f(*tensors)  # type:ignore[call-arg]\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<lambda>\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m1593\u001b[0m, in \u001b[35mwrapped_fn\u001b[0m\n",
      "    return tuple(\u001b[31mflat_fn\u001b[0m\u001b[1;31m(*args)\u001b[0m)\n",
      "                 \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\"\u001b[0m, line \u001b[35m187\u001b[0m, in \u001b[35mflat_fn\u001b[0m\n",
      "    tree_out = fn(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\graph_capture_wrappers.py\"\u001b[0m, line \u001b[35m1354\u001b[0m, in \u001b[35mfunctional_call\u001b[0m\n",
      "    out = mod(*args[params_len:], **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py\"\u001b[0m, line \u001b[35m843\u001b[0m, in \u001b[35mmodule_call_wrapper\u001b[0m\n",
      "    return \u001b[31mself.call_module\u001b[0m\u001b[1;31m(mod, forward, args, kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m1997\u001b[0m, in \u001b[35mcall_module\u001b[0m\n",
      "    return \u001b[31mTracer.call_module\u001b[0m\u001b[1;31m(self, m, forward, args, kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py\"\u001b[0m, line \u001b[35m560\u001b[0m, in \u001b[35mcall_module\u001b[0m\n",
      "    ret_val = forward(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py\"\u001b[0m, line \u001b[35m836\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    return _orig_module_call(mod, *args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[35m1775\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "    return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[35m1786\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\export\\_trace.py\"\u001b[0m, line \u001b[35m1906\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    tree_out = mod(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py\"\u001b[0m, line \u001b[35m843\u001b[0m, in \u001b[35mmodule_call_wrapper\u001b[0m\n",
      "    return \u001b[31mself.call_module\u001b[0m\u001b[1;31m(mod, forward, args, kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m1997\u001b[0m, in \u001b[35mcall_module\u001b[0m\n",
      "    return \u001b[31mTracer.call_module\u001b[0m\u001b[1;31m(self, m, forward, args, kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py\"\u001b[0m, line \u001b[35m560\u001b[0m, in \u001b[35mcall_module\u001b[0m\n",
      "    ret_val = forward(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py\"\u001b[0m, line \u001b[35m836\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    return _orig_module_call(mod, *args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[35m1775\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "    return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[35m1786\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \u001b[35m\"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\tools\\export_sthn_onnx.py\"\u001b[0m, line \u001b[35m89\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    _preds, four_point_disp = \u001b[31mself.net\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                              \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mimage1=image1,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<2 lines>...\n",
      "        \u001b[1;31mcorr_level=self.corr_level,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py\"\u001b[0m, line \u001b[35m843\u001b[0m, in \u001b[35mmodule_call_wrapper\u001b[0m\n",
      "    return \u001b[31mself.call_module\u001b[0m\u001b[1;31m(mod, forward, args, kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m1997\u001b[0m, in \u001b[35mcall_module\u001b[0m\n",
      "    return \u001b[31mTracer.call_module\u001b[0m\u001b[1;31m(self, m, forward, args, kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py\"\u001b[0m, line \u001b[35m560\u001b[0m, in \u001b[35mcall_module\u001b[0m\n",
      "    ret_val = forward(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py\"\u001b[0m, line \u001b[35m836\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    return _orig_module_call(mod, *args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[35m1775\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "    return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[35m1786\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \u001b[35m\"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py\"\u001b[0m, line \u001b[35m149\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    coords1 = self.get_flow_now_4(four_point_disp)\n",
      "  File \u001b[35m\"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py\"\u001b[0m, line \u001b[35m60\u001b[0m, in \u001b[35mget_flow_now_4\u001b[0m\n",
      "    if \u001b[31mtorch.isnan(points_new).any\u001b[0m\u001b[1;31m()\u001b[0m:\n",
      "       \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m1409\u001b[0m, in \u001b[35m__torch_function__\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py\"\u001b[0m, line \u001b[35m1479\u001b[0m, in \u001b[35m__torch_function__\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\_export\\non_strict_utils.py\"\u001b[0m, line \u001b[35m1066\u001b[0m, in \u001b[35m__torch_function__\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\sym_node.py\"\u001b[0m, line \u001b[35m538\u001b[0m, in \u001b[35mguard_bool\u001b[0m\n",
      "    r = self.evaluate()\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\sym_node.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mevaluate\u001b[0m\n",
      "    return \u001b[31mself.shape_env.evaluate_sym_node\u001b[0m\u001b[1;31m(self, size_oblivious)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py\"\u001b[0m, line \u001b[35m7233\u001b[0m, in \u001b[35mevaluate_sym_node\u001b[0m\n",
      "    return \u001b[31mself.evaluate_expr\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31msym_node.expr,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<3 lines>...\n",
      "        \u001b[1;31mfallback_value=fallback_value,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py\"\u001b[0m, line \u001b[35m7333\u001b[0m, in \u001b[35mevaluate_expr\u001b[0m\n",
      "    return \u001b[31mself._inner_evaluate_expr\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31morig_expr,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    ...<5 lines>...\n",
      "        \u001b[1;31mfallback_value,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\recording.py\"\u001b[0m, line \u001b[35m272\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return retlog(\u001b[31mfn\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m)\n",
      "                  \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py\"\u001b[0m, line \u001b[35m7356\u001b[0m, in \u001b[35m_inner_evaluate_expr\u001b[0m\n",
      "    return \u001b[31mself._evaluate_expr\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31morig_expr,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    ...<4 lines>...\n",
      "        \u001b[1;31mforcing_spec=forcing_spec,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py\"\u001b[0m, line \u001b[35m7574\u001b[0m, in \u001b[35m_evaluate_expr\u001b[0m\n",
      "    raise self._make_data_dependent_error(\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "\u001b[1;35mtorch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode\u001b[0m: \u001b[35mCould not guard on data-dependent expression Eq(u1, 1) (unhinted: Eq(u1, 1)).  (Size-like symbols: none)\n",
      "\n",
      "consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export\\non_strict_utils.py:1066 in __torch_function__)\n",
      "For more information, run with TORCH_LOGS=\"dynamic\"\n",
      "For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u1\"\n",
      "If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
      "For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n",
      "\n",
      "For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
      "\n",
      "The following call raised this error:\n",
      "  File \"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py\", line 60, in get_flow_now_4\n",
      "    if torch.isnan(points_new).any():\n",
      "\n",
      "\n",
      "The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\u001b[0m\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m198\u001b[0m, in \u001b[35m_run_module_as_main\u001b[0m\n",
      "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m88\u001b[0m, in \u001b[35m_run_code\u001b[0m\n",
      "  File \u001b[35m\"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\tools\\export_sthn_onnx.py\"\u001b[0m, line \u001b[35m265\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\tools\\export_sthn_onnx.py\"\u001b[0m, line \u001b[35m234\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31m_export_onnx\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mcoarse,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^\u001b[0m\n",
      "    ...<5 lines>...\n",
      "        \u001b[1;31mopset=args_cli.opset,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\tools\\export_sthn_onnx.py\"\u001b[0m, line \u001b[35m175\u001b[0m, in \u001b[35m_export_onnx\u001b[0m\n",
      "    \u001b[31mtorch.onnx.export\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^\u001b[0m\n",
      "    ...<6 lines>...\n",
      "        \u001b[1;31mdo_constant_folding=True,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\onnx\\__init__.py\"\u001b[0m, line \u001b[35m296\u001b[0m, in \u001b[35mexport\u001b[0m\n",
      "    return \u001b[31m_compat.export_compat\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^\u001b[0m\n",
      "    ...<20 lines>...\n",
      "        \u001b[1;31mlegacy_export_kwargs=legacy_export_kwargs,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mexport_compat\u001b[0m\n",
      "    onnx_program = _core.export(\n",
      "        model,\n",
      "    ...<11 lines>...\n",
      "        verbose=verbose,\n",
      "    )\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_flags.py\"\u001b[0m, line \u001b[35m23\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"C:\\Users\\Reza\\AppData\\Local\\Python\\pythoncore-3.13-64\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_core.py\"\u001b[0m, line \u001b[35m1385\u001b[0m, in \u001b[35mexport\u001b[0m\n",
      "    raise _errors.TorchExportError(\n",
      "    ...<7 lines>...\n",
      "    ) from first_error\n",
      "\u001b[1;35mtorch.onnx._internal.exporter._errors.TorchExportError\u001b[0m: \u001b[35mFailed to export the model with torch.export. \u001b[96mThis is step 1/3\u001b[0m of exporting the model to ONNX. Next steps:\n",
      "- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.\n",
      "- Debug `torch.export.export` and submit a PR to PyTorch.\n",
      "- Create an issue in the PyTorch GitHub repository against the \u001b[96m*torch.export*\u001b[0m component and attach the full error stack as well as reproduction scripts.\n",
      "\n",
      "## Exception summary\n",
      "\n",
      "<class 'torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode'>: Could not guard on data-dependent expression Eq(u1, 1) (unhinted: Eq(u1, 1)).  (Size-like symbols: none)\n",
      "\n",
      "consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export\\non_strict_utils.py:1066 in __torch_function__)\n",
      "For more information, run with TORCH_LOGS=\"dynamic\"\n",
      "For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u1\"\n",
      "If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
      "For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n",
      "\n",
      "For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
      "\n",
      "The following call raised this error:\n",
      "  File \"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\local_pipeline\\model\\network_cpu.py\", line 60, in get_flow_now_4\n",
      "    if torch.isnan(points_new).any():\n",
      "\n",
      "\n",
      "The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\n",
      "\n",
      "(Refer to the full stack trace above for more information.)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py -3.13 -m tools.export_sthn_onnx --pth \"js_models\\1536_one_stage\\STHN.pth\" --out_dir \"trt\\one_stage\" --stage coarse --resize_width 256 --corr_level 4 --iters 6 --database_size 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec2d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"d:\\Robotic Perceptoin Lab\\map-matching\\STHN-JetsonONX8\\tools\\export_sthn_onnx.py\"\u001b[0m, line \u001b[35m9\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    import local_pipeline\n",
      "\u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'local_pipeline'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py -3.13 -m tools.export_sthn_onnx --pth \"js_models\\1536_two_stages\\STHN.pth\" --out_dir \"trt\\two_stages\" --stage both --resize_width 256 --corr_level 4 --iters 6  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e0c0e",
   "metadata": {},
   "source": [
    "## Build TensorRT engines (requires TensorRT + `trtexec`)\n",
    "- Run these on the target GPU machine (e.g., Jetson).\n",
    "- If `trtexec` is not on your PATH, pass `--trtexec /full/path/to/trtexec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-stage (coarse)\n",
    "!py -3.13 tools/build_tensorrt_engine.py --onnx trt/one_stage/sthn_coarse.onnx --engine trt/one_stage/sthn_coarse_fp16.engine --fp16 --shapes \"min=image1:1x3x256x256,image2:1x3x256x256;opt=image1:1x3x256x256,image2:1x3x256x256;max=image1:1x3x256x256,image2:1x3x256x256\"\n",
    "\n",
    "# Two-stage (coarse + fine)\n",
    "!py -3.13 tools/build_tensorrt_engine.py --onnx trt/two_stages/sthn_coarse.onnx --engine trt/two_stages/sthn_coarse_fp16.engine --fp16 --shapes \"min=image1:1x3x256x256,image2:1x3x256x256;opt=image1:1x3x256x256,image2:1x3x256x256;max=image1:1x3x256x256,image2:1x3x256x256\"\n",
    "!py -3.13 tools/build_tensorrt_engine.py --onnx trt/two_stages/sthn_fine.onnx --engine trt/two_stages/sthn_fine_fp16.engine --fp16 --shapes \"min=image1_crop:1x3x256x256,image2:1x3x256x256;opt=image1_crop:1x3x256x256,image2:1x3x256x256;max=image1_crop:1x3x256x256,image2:1x3x256x256\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
