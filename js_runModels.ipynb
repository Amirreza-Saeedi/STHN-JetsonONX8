{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b5270b",
   "metadata": {},
   "source": [
    "*Note: Place this notebook in STHN directory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96c211",
   "metadata": {},
   "source": [
    "# TGM (Satellite to Thermal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyYAML transformers timm h5py torch torchvision faiss-cpu einops opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed123af",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff5c63",
   "metadata": {},
   "source": [
    "[my_eval_pix2pix.py](./global_pipeline/my_eval_pix2pix.py)\n",
    "- [Input Image](./global_pipeline/my_eval_pix2pix.py:69)\n",
    "- [Output Image](./global_pipeline/my_eval_pix2pix.py:94)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e83058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./global_pipeline/my_eval_pix2pix.py --resume=\"js_models/TGM_nocontrast/best_model.pth\" --dataset_name=none --datasets_folder ./maps --G_net unet --GAN_upsample bilinear --GAN_resize 1024 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730a305",
   "metadata": {},
   "source": [
    "## Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f25a8",
   "metadata": {},
   "source": [
    "[my_TGM_folder2folder.py](./global_pipeline/my_TGM_folder2folder.py)\n",
    "- [Input Folder](./global_pipeline/my_TGM_folder2folder.py:296)\n",
    "- [Output Folder](./global_pipeline/my_TGM_folder2folder.py:297)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ecc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py ./global_pipeline/my_TGM_folder2folder.py --resume=\"./maps/models/TGM_nocontrast/best_model.pth\" --dataset_name=none --datasets_folder ./maps --G_net unet --GAN_upsample bilinear --GAN_resize 1024 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab645e10",
   "metadata": {},
   "source": [
    "# SHN (Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kornia scikit-image wandb openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9a674",
   "metadata": {},
   "source": [
    "[my_myevaluate.py](./local_pipeline/my_myevaluate.py)\n",
    "- [Input Image](./local_pipeline/my_myevaluate.py:118)\n",
    "- [Output Image](./local_pipeline/my_myevaluate.py:119)\n",
    "- [Output Excel](./local_pipeline/my_myevaluate.py:168)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a0073",
   "metadata": {},
   "source": [
    "## One-Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(\"cuda mem allocated:\", torch.cuda.memory_allocated() // (1024**2), \"MB\")\n",
    "print(\"cuda mem reserved:\",   torch.cuda.memory_reserved()   // (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -3.13  ./local_pipeline/my_myevaluate.py --dataset_name none --eval_model js_models/1536_one_stage/STHN.pth --val_positive_dist_threshold 512 --lev0 --database_size 1536 --corr_level 4 --test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc4231",
   "metadata": {},
   "source": [
    "## Two-Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9d653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-01-04 03:16:40.004945976 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card1/device/vendor\"\u001b[m\n",
      "/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/local_pipeline/model/network.py:174: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "/home/rpl/.local/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/local_pipeline/model/network.py:203: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "‚úÖ Done for image 0\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "‚úÖ Done for image 1\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "‚úÖ Done for image 2\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "‚úÖ Done for image 3\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "‚úÖ Done for image 4\n",
      "torch.Size([1, 256, 64, 64]) torch.Size([1, 256, 64, 64])\n",
      "‚úÖ Done for image 5\n",
      "\n",
      "üìä Average processing time per image: 0.5516 sec\n",
      "üìÅ Saved all corner points to four_point_1_mul6.xlsx\n"
     ]
    }
   ],
   "source": [
    "!python3  ./local_pipeline/my_myevaluate.py --dataset_name none --eval_model js_models/1536_two_stages/STHN.pth --val_positive_dist_threshold 512 --lev0 --database_size 1536 --corr_level 4 --test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250e9d5",
   "metadata": {},
   "source": [
    "# TensorRT Conversion (STHN .pth ‚Üí ONNX ‚Üí TensorRT)\n",
    "- No `pycuda` is used (engine build via `trtexec`).\n",
    "- TensorRT `.engine` files are GPU/driver-specific: build them on the target machine (e.g., Jetson) with the same TensorRT version.\n",
    "- One-stage exports 1 ONNX/engine (coarse). Two-stage exports 2 ONNX/engines (coarse + fine). The crop/combine logic between stages remains in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd23a8f",
   "metadata": {},
   "source": [
    "## Export ONNX\n",
    "These commands export the weights from your `.pth` into ONNX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tools.export_sthn_onnx --pth \"js_models\\1536_one_stage\\STHN.pth\" --out_dir \"trt\\one_stage\" --stage coarse --resize_width 256 --corr_level 4 --iters 6 --database_size 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -3.13 -m tools.export_sthn_onnx --pth \"js_models\\1536_two_stages\\STHN.pth\" --out_dir \"trt\\two_stages\" --stage both --resize_width 256 --corr_level 4 --iters 6  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e0c0e",
   "metadata": {},
   "source": [
    "## Build TensorRT engines (requires TensorRT + `trtexec`)\n",
    "- Run these on the target GPU machine (e.g., Jetson).\n",
    "- If `trtexec` is not on your PATH, pass `--trtexec /full/path/to/trtexec`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f302449",
   "metadata": {},
   "source": [
    "### One-stage (coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ceecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tools.build_tensorrt_engine --onnx \"trt/one_stage/sthn_coarse.onnx\" --engine \"trt/one_stage/sthn_coarse_fp16.engine\" --fp16 --trtexec /usr/src/tensorrt/bin/trtexec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847e7e6",
   "metadata": {},
   "source": [
    "### Two-stage (coarse + fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adbc3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tools.build_tensorrt_engine --onnx \"trt/two_stages/sthn_coarse.onnx\" --engine \"trt/two_stages/sthn_coarse_fp16.engine\" --fp16 --trtexec /usr/src/tensorrt/bin/trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tools.build_tensorrt_engine --onnx \"trt/two_stages/sthn_fine.onnx\" --engine \"trt/two_stages/sthn_fine_fp16.engine\" --fp16 --trtexec /usr/src/tensorrt/bin/trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-stage (coarse)\n",
    "# !py -3.13 tools/build_tensorrt_engine.py --onnx trt/one_stage/sthn_coarse.onnx --engine trt/one_stage/sthn_coarse_fp16.engine --fp16 --shapes \"min=image1:1x3x256x256,image2:1x3x256x256;opt=image1:1x3x256x256,image2:1x3x256x256;max=image1:1x3x256x256,image2:1x3x256x256\"\n",
    "\n",
    "# # Two-stage (coarse + fine)\n",
    "# !py -3.13 tools/build_tensorrt_engine.py --onnx trt/two_stages/sthn_coarse.onnx --engine trt/two_stages/sthn_coarse_fp16.engine --fp16 --shapes \"min=image1:1x3x256x256,image2:1x3x256x256;opt=image1:1x3x256x256,image2:1x3x256x256;max=image1:1x3x256x256,image2:1x3x256x256\"\n",
    "# !py -3.13 tools/build_tensorrt_engine.py --onnx trt/two_stages/sthn_fine.onnx --engine trt/two_stages/sthn_fine_fp16.engine --fp16 --shapes \"min=image1_crop:1x3x256x256,image2:1x3x256x256;opt=image1_crop:1x3x256x256,image2:1x3x256x256;max=image1_crop:1x3x256x256,image2:1x3x256x256\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43be5c4",
   "metadata": {},
   "source": [
    "# TensorRT Inference (.engine)\n",
    "These use TensorRT engines directly (no `pycuda`).\n",
    "- One-stage: pass the coarse engine in `--eval_model`\n",
    "- Two-stage: pass both engines in `--eval_model` and `--eval_model_fine` and add `--two_stages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb31f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-01-02 11:53:46.974136665 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card1/device/vendor\"\u001b[m\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "[01/02/2026-11:53:47] [TRT] [E] [defaultAllocator.cpp::allocate::31] Error Code 1: Cuda Runtime (out of memory)\n",
      "[01/02/2026-11:53:47] [TRT] [E] [executionContext.cpp::ExecutionContext::565] Error Code 2: OutOfMemory (Requested size was 71320576 bytes.)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/./local_pipeline/my_myevaluate_trt.py\", line 107, in <module>\n",
      "    test(args)\n",
      "  File \"/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/./local_pipeline/my_myevaluate_trt.py\", line 42, in test\n",
      "    model = STHNTRT(\n",
      "  File \"/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/local_pipeline/model/network_trt.py\", line 86, in __init__\n",
      "    self.netG = IHNTRT(engine_coarse, image1_tensor_name=\"image1\")\n",
      "  File \"/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/local_pipeline/model/network_trt.py\", line 34, in __init__\n",
      "    self.engine = TrtEngine(engine_path)\n",
      "  File \"/home/rpl/Desktop/RPL/Map-Matching/STHN-JetsonONX8/local_pipeline/model/trt_engine.py\", line 49, in __init__\n",
      "    raise RuntimeError(f\"Failed to create TensorRT execution context: {engine_path}\")\n",
      "RuntimeError: Failed to create TensorRT execution context: trt/one_stage/sthn_coarse_fp16.engine\n"
     ]
    }
   ],
   "source": [
    "# One-stage TensorRT inference (coarse engine only)\n",
    "!python3 ./local_pipeline/my_myevaluate_trt.py --dataset_name none --eval_model trt/one_stage/sthn_coarse.engine --val_positive_dist_threshold 512 --lev0 --database_size 1536 --corr_level 4 --test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3065c15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(648.3709716796875, 732.7875366210938)\n",
      "tensor([[[439.5803, 512.3243],\n",
      "         [847.2935, 519.3723],\n",
      "         [453.0194, 950.8202],\n",
      "         [853.5908, 948.6334]]])\n",
      "‚úÖ Done for image 0\n",
      "(834.8838500976562, 753.2387084960938)\n",
      "tensor([[[ 489.9189,  414.2565],\n",
      "         [1162.2380,  405.4817],\n",
      "         [ 521.0660, 1089.0088],\n",
      "         [1166.3123, 1104.2079]]])\n",
      "‚úÖ Done for image 1\n",
      "(875.2380981445312, 712.0355224609375)\n",
      "tensor([[[799.8723, 634.5186],\n",
      "         [948.0734, 637.6548],\n",
      "         [805.1346, 788.9434],\n",
      "         [947.8721, 787.0253]]])\n",
      "‚úÖ Done for image 2\n",
      "(721.9774169921875, 722.1555786132812)\n",
      "tensor([[[ 386.0450,  380.0518],\n",
      "         [1044.4659,  397.2370],\n",
      "         [ 412.5507, 1058.5854],\n",
      "         [1044.8479, 1052.7480]]])\n",
      "‚úÖ Done for image 3\n",
      "(825.1072998046875, 750.33447265625)\n",
      "tensor([[[ 633.2064,  554.5209],\n",
      "         [1009.7736,  559.8675],\n",
      "         [ 645.2020,  945.6371],\n",
      "         [1012.2469,  941.3124]]])\n",
      "‚úÖ Done for image 4\n",
      "\n",
      "üìä Average processing time per image: 0.3184 sec\n",
      "üìÅ Saved all corner points to js_excels/predicted_trt.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Two-stage TensorRT inference (coarse + fine engines)\n",
    "!python3 ./local_pipeline/my_myevaluate_trt.py --dataset_name none --two_stages --eval_model trt/two_stages/sthn_coarse.engine --eval_model_fine trt/two_stages/sthn_fine.engine --val_positive_dist_threshold 512 --lev0 --database_size 1536 --corr_level 4 --test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
